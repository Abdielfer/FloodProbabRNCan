{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import myServices as ms\n",
    "import models as md\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_curve, auc, roc_auc_score, f1_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute ececution time do: \n",
    "# with timeit():\n",
    "#     # your code, e.g., \n",
    "class timeit(): \n",
    "    from datetime import datetime\n",
    "    def __enter__(self):\n",
    "        self.tic = self.datetime.now()\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print('runtime: {}'.format(self.datetime.now() - self.tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Score recorder ready:  Empty DataFrame\n",
      "Columns: [class_0, class_1, class_5, hyperParam]\n",
      "Index: []\n",
      "        TWI      TPI  DLSOL3R150  DLSOL4R150  DLSOL4R200  FAProx_01  \\\n",
      "0  13.59353 -0.15649     0.00315     0.00693     0.00671   85.00000   \n",
      "1   4.63718 -0.00439     0.00415     0.01119     0.01205   55.00000   \n",
      "2  11.71443 -0.41978     0.00092     0.00764     0.01099  125.00000   \n",
      "3  11.53867  0.02208     0.00000     0.00390     0.00264  114.01755   \n",
      "4   4.67272  0.19862     0.00379     0.00568     0.00567   93.00538   \n",
      "\n",
      "   FAProx_025  visibility     FAcc     slope  elevation  \n",
      "0    25.94224     0.04296  0.00028   9.59954  159.47165  \n",
      "1    11.18034     0.09969  0.00002   6.23723   94.90872  \n",
      "2    29.96665     0.07519  0.00003  10.90686  134.19553  \n",
      "3    72.53275     0.02210  0.00003   4.82951  178.78873  \n",
      "4    19.79899     0.20469  0.00003   8.11566   86.85973  \n",
      "Train balance\n",
      "(90225, {0: 'Class_count: 87838 (0.9735 percent)', 1: 'Class_count: 488 (0.0054 percent)', 5: 'Class_count: 1899 (0.0210 percent)'})\n",
      "X =  2\n",
      "[0.6506983917767343, 0.5, 0.6780300697400596, 100]\n",
      "current center__:  100.0  corrent best score__: 0.6780300697400596\n",
      "[0.659392362541304, 0.5, 0.6867228368902512, 110]\n",
      "current center__:  110.0  corrent best score__: 0.6867228368902512\n",
      "[0.6611357104282798, 0.5, 0.6878207542223006, 120]\n",
      "current center__:  120.0  corrent best score__: 0.6878207542223006\n",
      "[0.6486184336985474, 0.5, 0.6763776641131858, 130]\n",
      "current center__:  120.0  corrent best score__: 0.6878207542223006\n",
      "[0.6579450073683536, 0.5, 0.6826934535868701, 140]\n",
      "current center__:  120.0  corrent best score__: 0.6878207542223006\n",
      "[0.6693060017757275, 0.5, 0.6938421863008213, 150]\n",
      "current center__:  150.0  corrent best score__: 0.6938421863008213\n",
      "[0.6613584772679426, 0.5, 0.6871867536788716, 160]\n",
      "current center__:  150.0  corrent best score__: 0.6938421863008213\n"
     ]
    }
   ],
   "source": [
    "### Implement bets hiddenLayerSize exploration\n",
    "readPath = 'datasets/'\n",
    "trainingPath = readPath + 'basin3_Training.csv'\n",
    "validation = readPath + 'basin3_Test.csv'\n",
    "# dataset = pd.read_csv(trainingPath, index_col = None)\n",
    "params = {'random_state':50, 'hidden_layer_sizes': 2,\n",
    "                'early_stopping':True,'max_iter':200,'verbose':False,\n",
    "                'tol':0.00010,'validation_fraction':0.1,'warm_start':False}\n",
    "mlpc = md.implementingMLPCalssifier(trainingPath,'percentage',params)\n",
    "\n",
    "x_val,Y_val = ms.importDataSet(validation, 'percentage')\n",
    "X = x_val.copy()\n",
    "X.drop(['x_coord','y_coord'], axis=1, inplace=True)\n",
    "firstInterval = np.arange(100,1001,10)\n",
    "with timeit():\n",
    "    betHiddenLayerSize = mlpc.explore4BestHLSize(X,Y_val,firstInterval,'class_5',2)\n",
    "params['verbose'] = True\n",
    "params['hidden_layer_sizes'] = 880\n",
    "print(params)\n",
    "mlpc.restartMLPCalssifier(params)\n",
    "mlpc.fitMLPClassifier()\n",
    "print(mlpc.get_logsDic())\n",
    "prediction = ms.makePredictionToImportAsSHP(mlpClassifier, x_val, y_val, 'percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explore model capacity vs performance on unseening basin. ####\n",
    "\n",
    "'''\n",
    "hidden_layer_sizes=[100,100] earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6228742547616204, 1: 0.4999938829459356, 5: 0.6399651931990683}\n",
    "_____\n",
    "hidden_layer_sizes=[100,100] earlyStop False, Epochs = 45  ITÂ´s NOT Better!!\n",
    "_____\n",
    "hidden_layer_sizes=[1000,1000] earlyStop True: \n",
    "ROC_AUC one_vs_all:  {0: 0.6125170698989215, 1: 0.49991965918329867, 5: 0.6254008937160408}\n",
    "_____\n",
    "hidden_layer_sizes=(150,100,50), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.5637879768798603, 1: 0.499983295736978, 5: 0.5729145574443686}\n",
    "_____\n",
    "hidden_layer_sizes=(200,150,100,50), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6308461923938076, 1: 0.5000229432884631, 5: 0.6420998385622048}\n",
    "______\n",
    "hidden_layer_sizes=(200), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.654696371004719, 1: 0.5, 5: 0.6698573647187008}\n",
    "______\n",
    "hidden_layer_sizes=(278), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6739319215706956, 1: 0.5, 5: 0.687665244202922}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15007a490>, \n",
    "'activation': 'relu', 'n_iter': 42}\n",
    "_____\n",
    "ROC_AUC one_vs_all:  {0: 0.6434244793232012, 1: 0.4999990589147593, 5: 0.6580651931595939}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x14ffd44f0>, \n",
    "'activation': 'relu', 'hidden_layer_sizes': 279, 'n_iter': 26}\n",
    "______\n",
    "###.  BEST  #####\n",
    "'hidden_layer_sizes': 280\n",
    "ROC_AUC one_vs_all:  {0: 0.725413336714216, 1: 0.5, 5: 0.739561606898303}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x11115f880>, \n",
    "'activation': 'relu', 'n_iter': 32}\n",
    "____\n",
    "'hidden_layer_sizes': 281\n",
    "ROC_AUC one_vs_all:  {0: 0.687069575713219, 1: 0.5, 5: 0.7018609166432868}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150412280>, \n",
    "'activation': 'relu', 'n_iter': 32}\n",
    "_____\n",
    "'hidden_layer_sizes': 282\n",
    "ROC_AUC one_vs_all:  {0: 0.7091773125462395, 1: 0.5025620860825134, 5: 0.7231809060694995}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x14ffd43a0>, \n",
    "'activation': 'relu', 'hidden_layer_sizes': 282, 'n_iter': 32}\n",
    "______\n",
    "hidden_layer_sizes=(300), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6712874739483707, 1: 0.5, 5: 0.6903606434727337}\n",
    "_____\n",
    "hidden_layer_sizes=(350), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6634314979751396, 1: 0.5, 5: 0.6743297808926096}\n",
    "_____\n",
    "hidden_layer_sizes=(400), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6735994541470123, 1: 0.5, 5: 0.6886358464246944}\n",
    "____\n",
    "hidden_layer_sizes = 500\n",
    "ROC_AUC one_vs_all:  {0: 0.6413823150589926, 1: 0.5, 5: 0.6570811552402528}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15057a070>, \n",
    "'activation': 'relu', 'n_iter': 27}\n",
    "____\n",
    "hidden_layer_sizes = 600, \n",
    "ROC_AUC one_vs_all:  {0: 0.6629380284306443, 1: 0.5, 5: 0.678143405544148}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150143cd0>, \n",
    "'activation': 'relu', 'n_iter': 29}\n",
    "____\n",
    "hidden_layer_sizes' = 690\n",
    "ROC_AUC one_vs_all:  {0: 0.6720056649260504, 1: 0.5, 5: 0.684244579416157}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150528610>, \n",
    "'activation': 'relu', 'n_iter': 27}\n",
    "____\n",
    "###. to check####\n",
    "hidden_layer_sizes = 700\n",
    "ROC_AUC one_vs_all:  {0: 0.709241541871279, 1: 0.5, 5: 0.7225650719554007}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1501435e0>, \n",
    "'activation': 'relu', 'n_iter': 34}\n",
    "____\n",
    "hidden_layer_sizes = 710\n",
    "ROC_AUC one_vs_all:  {0: 0.6748179832958519, 1: 0.5, 5: 0.6911761628358285}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1500fea30>, '\n",
    "activation': 'relu', 'n_iter': 40}\n",
    "____\n",
    "hidden_layer_sizes = 800,\n",
    "ROC_AUC one_vs_all:  {0: 0.6665697027100255, 1: 0.5, 5: 0.6828522971609348}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150145f70>,\n",
    "'activation': 'relu', 'n_iter': 34}\n",
    "____\n",
    "hidden_layer_sizes = 900\n",
    "ROC_AUC one_vs_all:  {0: 0.6361267478738225, 1: 0.5, 5: 0.6481332615660654}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1500fe460>, \n",
    "'activation': 'relu','n_iter': 33}\n",
    "_____\n",
    "hidden_layer_sizes = 1000\n",
    "ROC_AUC one_vs_all:  {0: 0.6714374119850677, 1: 0.5, 5: 0.6856021298501717}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15007a640>, \n",
    "'activation': 'relu', 'n_iter': 36}\n",
    "'''  \n",
    "prediction.to_csv(('outputs/'+ 'MLP_basin1Light_firstResult_HL280.csv'),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and manipulating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning basin1DataSet \n",
    "dataSetPath = 'datasets/datasetBasin1.csv'\n",
    "basinDataSet = pd.read_csv(dataSetPath, index_col = None)\n",
    "# basin1Light = pd.read_csv('datasetBasin1_NoDataFree.csv', index_col = None)\n",
    "# print(basinDataSet.info())\n",
    "# basinDataSet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['percentage','DLSOL5R200', 'DLSOL4R150', 'DLSOL5R150']\n",
    "for col in colNames: \n",
    "    basinDataSet[col].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.dropna(subset=['slope'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.drop(['fid'], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOrmalize Flow Accumulation\n",
    "basinDataSet['FAcc'] = (basinDataSet['FAcc']- basinDataSet['FAcc'].min())/(basinDataSet['FAcc'].max()-basinDataSet['FAcc'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing QGIS NoData value(-9999) with 0 \n",
    "repalcer  = basinDataSet['FAProx_01'].to_numpy()\n",
    "basinDataSet['FAProx_01'] = [0 if repalcer[j] == -9999 else repalcer[j] for j in range(len(repalcer))]                                                                                                                         \n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform a column datatype\n",
    "repalcer  = basinDataSet['percentage'].to_numpy().astype('int')\n",
    "basinDataSet.loc[:,'percentage'] = repalcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.to_csv('datasets/basin1_FirstFeatureSet_Clean.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS.head(5)\n",
    "s = {}\n",
    "s['Datas'] = ds\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportional Spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X,Y = ms.importDataSet('datasets/basin1_FirstFeatureSet_Clean.csv', 'percentage')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=50)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    print(\"TRAIN:\", train_index.size, \"TEST:\", test_index.size)\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = Y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = Y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describing training set\n",
    "print(len(X_train['elevation']), len(y_train) )\n",
    "trainCount = Counter(y_train)\n",
    "print(trainCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####    Creating training set     #####\n",
    "X_train.loc[:,'percentage'] = y_train\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing coordinates from training set\n",
    "X_train.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('datasets/basin1_FirstFeatureSet_Clean_Training.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####. Creating Test set\n",
    "print(X_test.head())\n",
    "X_test.loc[:,'percentage'] = y_test\n",
    "print(X_test.head())\n",
    "print(X_test.info())\n",
    "testCount = Counter(X_test['percentage'])\n",
    "print(f\"testCount:  {testCount}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('datasets/basin1_FirstFeatureSet_Clean_Test.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This proportions are the reason why a sample_weight of 0.01 for the majority class give best results for regression\n",
    "totalTrain = sum([trainCount[0], trainCount[1], trainCount[5]]) \n",
    "totalValidation = sum([testCount[0], testCount[1], testCount[5]])\n",
    "print(f\"total Train samples: {totalTrain},  total Validation samples: {totalValidation}\")\n",
    "print(\"Summary of traning and test dataset class balance\")\n",
    "print(f\"Training Set:\", '\\n', \"Class 0: %.3f\" %(trainCount[0]/totalTrain), \" Class 1: %.4f\" %(trainCount[1]/totalTrain), \"Class 5: %.4f\"%(trainCount[5]/totalTrain))\n",
    "print(\"Testing Set:\", '\\n', \"Class 0: %.3f\" %(testCount[0]/totalValidation),\" Class 1: %.4f\" %(testCount[1]/totalValidation),  \"Class 5: %.4f\"%(testCount[5]/totalValidation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms.loadModel('./outputs/2022-08-05/00-35-58/2208050035.pkl')\n",
    "dataSetToSave = ms.makePredictionToImportAsSHP(csvName, model, X, Y, 'percentage')\n",
    "print(dataSetToSave.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining dataSets to build AllVsOne_training and OneVsAll_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat datasets\n",
    "sourceFileForDatasets = 'datasets/'\n",
    "allDataSetsNames = ['basin1Light_Clean.csv', 'basin2_CleanDataSet.csv', 'basin3_CleanDataSet_copy.csv','basin4_CleanDataSet.csv','basin5_CleanDataSet.csv']\n",
    "# OneVsAllDataSetName = 'basin1Light_Clean.csv'\n",
    "\n",
    "for datasetForTest in allDataSetsNames:\n",
    "    filename, file_extension = os.path.splitext(datasetForTest)\n",
    "    newListOfNames = [s for s in allDataSetsNames if s != datasetForTest]\n",
    "    allDataSetsFileName = 'allVs_'+ filename +'_Training'\n",
    "    DFToConcatAll = pd.DataFrame()\n",
    "#     DFToConcatAll = pd.read_csv((sourceFileForDatasets+datasetForTest), index_col = None)\n",
    "#     print(DFToConcatAll.head())\n",
    "    for datasets in newListOfNames:\n",
    "        DFToConcatAll = pd.concat([DFToConcatAll, pd.read_csv((sourceFileForDatasets+datasets), index_col = None)])\n",
    "    nameToSafe = sourceFileForDatasets+allDataSetsFileName+file_extension\n",
    "    DFToConcatAll.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "    DFToConcatAll.to_csv(nameToSafe, index=None)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning datasets: Removing not usefull variables from All_VS_ONE \n",
    "readPath = 'datasets/'\n",
    "destiationPath = 'datasets/dataset4MLP/'\n",
    "datasetNamelist = ['allVs_basin1Light_Clean_Training.csv', 'allVs_basin2_CleanDataSet_Training.csv','allVs_basin3_CleanDataSet_copy_Training.csv',\n",
    "'allVs_basin4_CleanDataSet_Training.csv','allVs_basin5_CleanDataSet_Training.csv','basin1Light_Clean_VsAll_Test.csv',\n",
    "'basin2_CleanDataSet_VsAll_Test.csv','basin3_CleanDataSet_copy_VsAll_Test.csv', 'basin4_CleanDataSet_VsAll_Test.csv', \n",
    "'basin5_CleanDataSet_VsAll_Test.csv']\n",
    "featuresToDelete = ['TPI','TWI']\n",
    "for i in datasetNamelist:\n",
    "    path = readPath + i \n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    basinDataSet.drop(featuresToDelete, axis=1, inplace=True)\n",
    "    savePath = destiationPath + 'MLP_'+i\n",
    "    basinDataSet.to_csv(savePath, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###. Build dataset subset for MLP test (Only first 150K samples)\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "datasetNamelist = ['MLP_allVs_basin1Light_Clean_Training.csv','MLP_basin1Light_Clean_VsAll_Test.csv']\n",
    "for i in datasetNamelist:\n",
    "    path = readPath + i \n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    Y = np.array(basinDataSet['percentage'])\n",
    "    count,_ = md.listClassCountPercent(Y)\n",
    "    basinDataSet.drop(basinDataSet.loc[150000:count].index,axis=0,inplace=True)\n",
    "    savePath = readPath + 'reduced_'+i\n",
    "    basinDataSet.to_csv(savePath, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring datasets\n",
    "dataset = ['reduced_MLP_allVs_basin1Light_Clean_Training.csv','reduced_MLP_basin1Light_Clean_VsAll_Test.csv']\n",
    "for i in dataset:\n",
    "    path = readPath + i \n",
    "    print(path)\n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    print(basinDataSet.head())\n",
    "    Y = np.array(basinDataSet['percentage'])\n",
    "    print(md.listClassCountPercent(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "def training(model, x_train,y_train, x_valid, y_valid, epochs, threshold):\n",
    "    for i in epochs:\n",
    "        model.fit(x_train,y_train)\n",
    "        if i == threshold:\n",
    "            md.computeClassificationMetrics(mlpClassifier,X,y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.  Training TEST\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "trainingPath = readPath + 'MLP_allVs_basin1Light_Clean_Training.csv'\n",
    "# dataset = pd.read_csv(trainingPath, index_col = None)\n",
    "args = {'eStop': True}\n",
    "mlpc = md.implementingMLPCalssifier(trainingPath,'percentage',args)\n",
    "mlpc.fitMLPClassifier()\n",
    "mlpc.plotLossBehaviour()\n",
    "\n",
    "mlpClassifier = mlpc.getMLPClassifier()\n",
    "\n",
    "#Validating un unseen datase\n",
    "validation = readPath + 'MLP_basin1Light_Clean_VsAll_Test.csv'\n",
    "x_val,y_val = ms.importDataSet(validation, 'percentage')\n",
    "prediction = ms.makePredictionToImportAsSHP(mlpClassifier, x_val, y_val, 'percentage')\n",
    "\n",
    "## Compute metrics\n",
    "X = x_val.copy()\n",
    "X.drop(['x_coord','y_coord'], axis=1, inplace=True)\n",
    "metrics = md.computeClassificationMetrics(mlpClassifier,X,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlpc.get_logsDic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(('outputs/'+ 'MLP_basin1Light_firstResult_HL280.csv'),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myServices as ms\n",
    "import models as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement bets hiddenLayerSize exploration\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "trainingPath = readPath + 'MLP_allVs_basin1Light_Clean_Training.csv'\n",
    "validation = readPath + 'MLP_basin1Light_Clean_VsAll_Test.csv'\n",
    "# dataset = pd.read_csv(trainingPath, index_col = None)\n",
    "params = {'random_state':50, 'hidden_layer_sizes': 2,\n",
    "                'early_stopping':False,'max_iter':2,'verbose':True,\n",
    "                'tol':0.00010,'validation_fraction':0.1,'warm_start':False}\n",
    "mlpc = md.implementingMLPCalssifier(trainingPath,'percentage',params)\n",
    "\n",
    "x_val,Y_val = ms.importDataSet(validation, 'percentage')\n",
    "X = x_val.copy()\n",
    "X.drop(['x_coord','y_coord'], axis=1, inplace=True)\n",
    "firstInterval = np.arange(100,1009,100)\n",
    "mlpc.explore4BestHLSize(X,Y_val,firstInterval,'5',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controled sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = pd.read_csv('datasets/basin2 _Training.csv', index_col = None)\n",
    "print(DS.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(DS['FAProx_01']) # , , DS['elevation'], DS['disToRiv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling appliying class selection by rule:\n",
    "\n",
    "# RULE1: Select point at a distance to river less than 300m. \n",
    "\n",
    "# # newDS = pseudoClassCreation(DS, \"distanceToRiver\", 300, 2)\n",
    "def pseudoClassCreation(dataset, conditionVariable, threshold, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Replace <targetClass> by  <pseudoClass> where <conditionVariable >= threshold>. \n",
    "    Return:\n",
    "      dataset with new classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    conditionVar = (np.array(dataset[conditionVariable])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ pseudoClass if conditionVar[j] >= threshold \n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "def revertPseudoClassCreation(dataset, originalClass, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Restablich  <targetClass> with <originalClass> where <targetClassName == pseudoClass>. \n",
    "    Return:\n",
    "      dataset with original classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ originalClass if actualTarget[j] == pseudoClass\n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "\n",
    "print(Counter(X_train['percentage']))\n",
    "newDS = pseudoClassCreation(X_train, 'disToRiv', 200, 2, 'percentage')\n",
    "y = newDS['percentage']\n",
    "newDS.drop(['percentage'], axis=1, inplace = True)\n",
    "x_res,y_res = ms.randomUndersampling(newDS, y, )\n",
    "x_res['percentage'] = y_res\n",
    "# newDatase = revertPseudoClassCreation(x_res, 0, 2, 'percentage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res.to_csv('basin1ControlClass0Sampling4Class_ToSHP.csv',index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import dataset to describe\n",
    "DS= pd.read_csv('datasets/basin4_Training.csv', index_col=None)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.drop(['x_coord','y_coord'], axis = 1, inplace=True)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAcc vs Labels\n",
    "targets = DS['percentage']\n",
    "FAcc = original['FAcc']\n",
    "FAcc_norm = DS['FAcc_norm']\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13,4), sharey=True)\n",
    "fig.text(-0.02, 0.5, 'labels', va='center', rotation='vertical')\n",
    "fig.text(0.5, 1, 'Flow accumulation vs labels distribution', ha ='center')\n",
    "axs[0].scatter(FAcc,targets)\n",
    "# axs[0].set_title(\"Facc\")\n",
    "axs[0].set(xlabel='a) Flow Accumulation')\n",
    "axs[1].scatter(FAcc_norm,targets)\n",
    "# axs[1].set_title(\"FAcc_norm\")\n",
    "axs[1].set(xlabel='b) Flow Accumulation estandardized')\n",
    "plt.rcParams['font.size'] = '20'\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Plot all features vs labels\n",
    "# 'disToRiv', 'TWI', 'TPI', 'slope', 'elevation',\n",
    "\n",
    "targets = DS['percentage']\n",
    "# targets = np.where(targets == 5,2,targets)\n",
    "\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['LDSOL4R150']\n",
    "DLSOL5R150 = DS['LDSOL5R150']\n",
    "DLSOL5R200 = DS['LDSOL5R200']\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "\n",
    "fig, axs = plt.subplots(4,3, figsize=(13, 8), sharey=True)\n",
    "fig.supylabel('Labels')\n",
    "plt.rcParams['font.size'] = '15'\n",
    "plt.yticks([0,1,5])\n",
    "\n",
    "'''\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "'''\n",
    "axs[0, 0].scatter(E,targets)\n",
    "axs[0, 0].set_title(\"Elevation\")\n",
    "axs[1, 0].scatter(slope,targets)\n",
    "axs[1, 0].set_title(\"Slope\")\n",
    "axs[2, 0].scatter(FAcc,targets)\n",
    "axs[2, 0].set_title(\"Flow accumulation\")\n",
    "axs[3, 0].scatter(TWI,targets)\n",
    "axs[3, 0].set_title(\"TWI\")\n",
    "\n",
    "'''\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['DLSOL4R150']\n",
    "DLSOL5R150 = DS['DLSOL5R150']\n",
    "DLSOL5R200 = DS['DLSOL5R200']\n",
    "'''\n",
    "axs[0, 1].scatter(TPI,targets)\n",
    "axs[0, 1].set_title('TPI')\n",
    "axs[1, 1].scatter(DLSOL4R150,targets)\n",
    "axs[1, 1].set_title(\"DLSOL4R150\")\n",
    "axs[2, 1].scatter(DLSOL5R150,targets)\n",
    "axs[2, 1].set_title(\"DLSOL5R150\")\n",
    "axs[3, 1].scatter(DLSOL5R200,targets)\n",
    "axs[3, 1].set_title(\"DLSOL5R200\")\n",
    "\n",
    "'''\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "'''\n",
    "axs[0, 2].scatter(FAProx_01,targets)\n",
    "axs[0, 2].set_title('FAProx_01')\n",
    "axs[1, 2].scatter(FAProx_025,targets)\n",
    "axs[1, 2].set_title(\"FAProx_025\")\n",
    "axs[2, 2].scatter(visibility,targets)\n",
    "axs[2, 2].set_title(\"Visibility\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.head())\n",
    "#  Return a dataset with the rows corresponding to the index where condition in DS.columName is valid. \n",
    "dsArray = DS[DS.percentage != 0] \n",
    "print(dsArray.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.pairplot(DS, hue = 'percentage', diag_kind = 'kde', \n",
    "             plot_kws = {'alpha': 0.8, 's': 100},\n",
    "             height = 4, corner=True, palette = \"Set2\")# vars = ['life_exp', 'log_pop', 'log_gdp_per_cap'],\n",
    "\n",
    "# sns.pairplot(DS, hue=\"percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####. Covariance Matrix\n",
    "sns.set(font_scale=0.7)\n",
    "matrix = DS.corr().round(2)\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.set_figsize=(25,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(criterion='entropy', random_state = 50)\n",
    "x_train,y_train = ms.importDataSet('basin1Train.csv', 'percentage')\n",
    "classifier = OneVsRestClassifier(estimator).fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ms.loadModel('./outputs/2022-08-05/11-01-57/2208051101.pkl')\n",
    "x_test,y_test = ms.importDataSet('basin1Test.csv', 'percentage')\n",
    "\n",
    "x_test = ms.removeCoordinatesFromDataSet(x_test)\n",
    "\n",
    "# y_prob = classifier.predict_proba(x_test)\n",
    "#print(np.unique(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.plot_ROC_AUC_OneVsRest(classifier, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,y_test = ms.importDataSet('./bestModels/Classifier/10-18-08/2208051018prediction_basin1Test.csv', 'prediction')\n",
    "unique, count = np.unique(y_test, return_counts=True)\n",
    "total = count.sum()\n",
    "print(total)\n",
    "percent = np.round(np.zeros_like(unique).astype('float16'),3)\n",
    "print('values, counts , percent')\n",
    "for i in range(len(unique)):    \n",
    "   percent[i] = (count[i]/total)*100\n",
    "   print(unique[i],\"\\t\", count[i], percent[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhiteBoxTool applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whiteboxApplications as wbtapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = '/Users/abdielfer/DESS/Internship2022/RNCanWork/FloodMaps/WBTWorkDir'\n",
    "rtools = wbtapp.rasterTools(wdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    rtools.gaussianFilter('basin1_FAccDInfLogGauss2.tif', 'basin1_FAccDInfLogGauss2.tif', sigma = 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c444c068df4da933f83035bc75b2ed5aea8e7fb8bc113357e2f14ee7cca6a8fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
