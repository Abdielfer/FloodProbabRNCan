{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import myServices as ms\n",
    "import models as md\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_curve, auc, roc_auc_score, f1_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute ececution time do: \n",
    "# with timeit():\n",
    "#     # your code, e.g., \n",
    "class timeit(): \n",
    "    from datetime import datetime\n",
    "    def __enter__(self):\n",
    "        self.tic = self.datetime.now()\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print('runtime: {}'.format(self.datetime.now() - self.tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explore model capacity vs performance on unseening basin. ####\n",
    "\n",
    "'''\n",
    "hidden_layer_sizes=[100,100] earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6228742547616204, 1: 0.4999938829459356, 5: 0.6399651931990683}\n",
    "_____\n",
    "hidden_layer_sizes=[100,100] earlyStop False, Epochs = 45  ITÂ´s NOT Better!!\n",
    "_____\n",
    "hidden_layer_sizes=[1000,1000] earlyStop True: \n",
    "ROC_AUC one_vs_all:  {0: 0.6125170698989215, 1: 0.49991965918329867, 5: 0.6254008937160408}\n",
    "_____\n",
    "hidden_layer_sizes=(150,100,50), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.5637879768798603, 1: 0.499983295736978, 5: 0.5729145574443686}\n",
    "_____\n",
    "hidden_layer_sizes=(200,150,100,50), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6308461923938076, 1: 0.5000229432884631, 5: 0.6420998385622048}\n",
    "______\n",
    "hidden_layer_sizes=(200), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.654696371004719, 1: 0.5, 5: 0.6698573647187008}\n",
    "______\n",
    "hidden_layer_sizes=(278), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6739319215706956, 1: 0.5, 5: 0.687665244202922}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15007a490>, \n",
    "'activation': 'relu', 'n_iter': 42}\n",
    "_____\n",
    "ROC_AUC one_vs_all:  {0: 0.6434244793232012, 1: 0.4999990589147593, 5: 0.6580651931595939}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x14ffd44f0>, \n",
    "'activation': 'relu', 'hidden_layer_sizes': 279, 'n_iter': 26}\n",
    "______\n",
    "###.  BEST  #####\n",
    "'hidden_layer_sizes': 280\n",
    "ROC_AUC one_vs_all:  {0: 0.725413336714216, 1: 0.5, 5: 0.739561606898303}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x11115f880>, \n",
    "'activation': 'relu', 'n_iter': 32}\n",
    "____\n",
    "'hidden_layer_sizes': 281\n",
    "ROC_AUC one_vs_all:  {0: 0.687069575713219, 1: 0.5, 5: 0.7018609166432868}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150412280>, \n",
    "'activation': 'relu', 'n_iter': 32}\n",
    "_____\n",
    "'hidden_layer_sizes': 282\n",
    "ROC_AUC one_vs_all:  {0: 0.7091773125462395, 1: 0.5025620860825134, 5: 0.7231809060694995}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x14ffd43a0>, \n",
    "'activation': 'relu', 'hidden_layer_sizes': 282, 'n_iter': 32}\n",
    "______\n",
    "hidden_layer_sizes=(300), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6712874739483707, 1: 0.5, 5: 0.6903606434727337}\n",
    "_____\n",
    "hidden_layer_sizes=(350), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6634314979751396, 1: 0.5, 5: 0.6743297808926096}\n",
    "_____\n",
    "hidden_layer_sizes=(400), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6735994541470123, 1: 0.5, 5: 0.6886358464246944}\n",
    "____\n",
    "hidden_layer_sizes = 500\n",
    "ROC_AUC one_vs_all:  {0: 0.6413823150589926, 1: 0.5, 5: 0.6570811552402528}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15057a070>, \n",
    "'activation': 'relu', 'n_iter': 27}\n",
    "____\n",
    "hidden_layer_sizes = 600, \n",
    "ROC_AUC one_vs_all:  {0: 0.6629380284306443, 1: 0.5, 5: 0.678143405544148}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150143cd0>, \n",
    "'activation': 'relu', 'n_iter': 29}\n",
    "____\n",
    "hidden_layer_sizes' = 690\n",
    "ROC_AUC one_vs_all:  {0: 0.6720056649260504, 1: 0.5, 5: 0.684244579416157}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150528610>, \n",
    "'activation': 'relu', 'n_iter': 27}\n",
    "____\n",
    "###. to check####\n",
    "hidden_layer_sizes = 700\n",
    "ROC_AUC one_vs_all:  {0: 0.709241541871279, 1: 0.5, 5: 0.7225650719554007}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1501435e0>, \n",
    "'activation': 'relu', 'n_iter': 34}\n",
    "____\n",
    "hidden_layer_sizes = 710\n",
    "ROC_AUC one_vs_all:  {0: 0.6748179832958519, 1: 0.5, 5: 0.6911761628358285}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1500fea30>, '\n",
    "activation': 'relu', 'n_iter': 40}\n",
    "____\n",
    "hidden_layer_sizes = 800,\n",
    "ROC_AUC one_vs_all:  {0: 0.6665697027100255, 1: 0.5, 5: 0.6828522971609348}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150145f70>,\n",
    "'activation': 'relu', 'n_iter': 34}\n",
    "____\n",
    "hidden_layer_sizes = 900\n",
    "ROC_AUC one_vs_all:  {0: 0.6361267478738225, 1: 0.5, 5: 0.6481332615660654}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1500fe460>, \n",
    "'activation': 'relu','n_iter': 33}\n",
    "_____\n",
    "hidden_layer_sizes = 1000\n",
    "ROC_AUC one_vs_all:  {0: 0.6714374119850677, 1: 0.5, 5: 0.6856021298501717}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15007a640>, \n",
    "'activation': 'relu', 'n_iter': 36}\n",
    "'''  \n",
    "prediction.to_csv(('outputs/'+ 'MLP_basin1Light_firstResult_HL280.csv'),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and manipulating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning basin1DataSet \n",
    "readSetPath = 'datasets/dataset4MLP/'\n",
    "importName = 'MLP_basin2_Test.csv'\n",
    "saveDatasetPath = 'datasets/datasets4MLP_Binary/'\n",
    "basinDataSet = pd.read_csv((readSetPath+importName), index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['percentage','DLSOL5R200', 'DLSOL4R150', 'DLSOL5R150']\n",
    "for col in colNames: \n",
    "    basinDataSet[col].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.dropna(subset=['slope'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.drop(['fid'], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOrmalize Flow Accumulation\n",
    "basinDataSet['FAcc'] = (basinDataSet['FAcc']- basinDataSet['FAcc'].min())/(basinDataSet['FAcc'].max()-basinDataSet['FAcc'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing QGIS NoData value(-9999) with 0 \n",
    "repalcer  = basinDataSet['FAProx_01'].to_numpy()\n",
    "basinDataSet['FAProx_01'] = [0 if repalcer[j] == -9999 else repalcer[j] for j in range(len(repalcer))]                                                                                                                         \n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform a column datatype\n",
    "repalcer  = basinDataSet['percentage'].to_numpy().astype('int')\n",
    "basinDataSet.loc[:,'percentage'] = repalcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make binary Dataset ###\n",
    "# keep class_0 and replace with 1 all other classes. \n",
    "basinDataSet = ms.makeBinary(basinDataSet,'percentage',0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DLSOL4R150</th>\n",
       "      <th>DLSOL5R150</th>\n",
       "      <th>DLSOL5R200</th>\n",
       "      <th>FAProx_01</th>\n",
       "      <th>FAProx_025</th>\n",
       "      <th>FAcc</th>\n",
       "      <th>visibility</th>\n",
       "      <th>slope</th>\n",
       "      <th>elevation</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>67421.000000</td>\n",
       "      <td>6.742100e+04</td>\n",
       "      <td>67421.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>230.110872</td>\n",
       "      <td>60.480869</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.046295</td>\n",
       "      <td>6.594304</td>\n",
       "      <td>133.406209</td>\n",
       "      <td>359623.789457</td>\n",
       "      <td>5.270242e+06</td>\n",
       "      <td>0.015856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>168.684296</td>\n",
       "      <td>44.524278</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.045242</td>\n",
       "      <td>5.236521</td>\n",
       "      <td>31.981416</td>\n",
       "      <td>969.663972</td>\n",
       "      <td>1.216866e+03</td>\n",
       "      <td>0.124918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.061920</td>\n",
       "      <td>358213.000000</td>\n",
       "      <td>5.266260e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>92.195440</td>\n",
       "      <td>24.041630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>2.742950</td>\n",
       "      <td>120.027690</td>\n",
       "      <td>358938.000000</td>\n",
       "      <td>5.269398e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>199.624650</td>\n",
       "      <td>51.739730</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.031870</td>\n",
       "      <td>5.418560</td>\n",
       "      <td>138.648450</td>\n",
       "      <td>359418.000000</td>\n",
       "      <td>5.270228e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>336.340610</td>\n",
       "      <td>88.955050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.057020</td>\n",
       "      <td>9.067720</td>\n",
       "      <td>152.262250</td>\n",
       "      <td>360098.000000</td>\n",
       "      <td>5.271063e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>894.427190</td>\n",
       "      <td>246.375730</td>\n",
       "      <td>1.000150</td>\n",
       "      <td>0.395220</td>\n",
       "      <td>50.067170</td>\n",
       "      <td>227.877210</td>\n",
       "      <td>364170.000000</td>\n",
       "      <td>5.272723e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DLSOL4R150    DLSOL5R150    DLSOL5R200     FAProx_01    FAProx_025  \\\n",
       "count  67421.000000  67421.000000  67421.000000  67421.000000  67421.000000   \n",
       "mean       0.001832      0.003782      0.003789    230.110872     60.480869   \n",
       "std        0.002676      0.003907      0.003294    168.684296     44.524278   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000860     92.195440     24.041630   \n",
       "50%        0.000000      0.003040      0.003300    199.624650     51.739730   \n",
       "75%        0.003720      0.005950      0.005910    336.340610     88.955050   \n",
       "max        0.013090      0.020200      0.016340    894.427190    246.375730   \n",
       "\n",
       "               FAcc    visibility         slope     elevation        x_coord  \\\n",
       "count  67421.000000  67421.000000  67421.000000  67421.000000   67421.000000   \n",
       "mean       0.002449      0.046295      6.594304    133.406209  359623.789457   \n",
       "std        0.037738      0.045242      5.236521     31.981416     969.663972   \n",
       "min        0.000000      0.000040      0.003640      0.061920  358213.000000   \n",
       "25%        0.000000      0.017880      2.742950    120.027690  358938.000000   \n",
       "50%        0.000010      0.031870      5.418560    138.648450  359418.000000   \n",
       "75%        0.000050      0.057020      9.067720    152.262250  360098.000000   \n",
       "max        1.000150      0.395220     50.067170    227.877210  364170.000000   \n",
       "\n",
       "            y_coord    percentage  \n",
       "count  6.742100e+04  67421.000000  \n",
       "mean   5.270242e+06      0.015856  \n",
       "std    1.216866e+03      0.124918  \n",
       "min    5.266260e+06      0.000000  \n",
       "25%    5.269398e+06      0.000000  \n",
       "50%    5.270228e+06      0.000000  \n",
       "75%    5.271063e+06      0.000000  \n",
       "max    5.272723e+06      1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basinDataSet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportName = 'MLPBimary_basin2_Test.csv'\n",
    "basinDataSet.to_csv((saveDatasetPath+exportName), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS.head(5)\n",
    "s = {}\n",
    "s['Datas'] = ds\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportional Spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X,Y = ms.importDataSet('datasets/basin1_FirstFeatureSet_Clean.csv', 'percentage')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=50)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    print(\"TRAIN:\", train_index.size, \"TEST:\", test_index.size)\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = Y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = Y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describing training set\n",
    "print(len(X_train['elevation']), len(y_train) )\n",
    "trainCount = Counter(y_train)\n",
    "print(trainCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####    Creating training set     #####\n",
    "X_train.loc[:,'percentage'] = y_train\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing coordinates from training set\n",
    "X_train.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('datasets/basin1_FirstFeatureSet_Clean_Training.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####. Creating Test set\n",
    "print(X_test.head())\n",
    "X_test.loc[:,'percentage'] = y_test\n",
    "print(X_test.head())\n",
    "print(X_test.info())\n",
    "testCount = Counter(X_test['percentage'])\n",
    "print(f\"testCount:  {testCount}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('datasets/basin1_FirstFeatureSet_Clean_Test.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This proportions are the reason why a sample_weight of 0.01 for the majority class give best results for regression\n",
    "totalTrain = sum([trainCount[0], trainCount[1], trainCount[5]]) \n",
    "totalValidation = sum([testCount[0], testCount[1], testCount[5]])\n",
    "print(f\"total Train samples: {totalTrain},  total Validation samples: {totalValidation}\")\n",
    "print(\"Summary of traning and test dataset class balance\")\n",
    "print(f\"Training Set:\", '\\n', \"Class 0: %.3f\" %(trainCount[0]/totalTrain), \" Class 1: %.4f\" %(trainCount[1]/totalTrain), \"Class 5: %.4f\"%(trainCount[5]/totalTrain))\n",
    "print(\"Testing Set:\", '\\n', \"Class 0: %.3f\" %(testCount[0]/totalValidation),\" Class 1: %.4f\" %(testCount[1]/totalValidation),  \"Class 5: %.4f\"%(testCount[5]/totalValidation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms.loadModel('./outputs/2022-08-05/00-35-58/2208050035.pkl')\n",
    "dataSetToSave = ms.makePredictionToImportAsSHP(csvName, model, X, Y, 'percentage')\n",
    "print(dataSetToSave.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining dataSets to build AllVsOne_training and OneVsAll_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat datasets\n",
    "sourceFileForDatasets = 'datasets/'\n",
    "allDataSetsNames = ['basin1Light_Clean.csv', 'basin2_CleanDataSet.csv', 'basin3_CleanDataSet_copy.csv','basin4_CleanDataSet.csv','basin5_CleanDataSet.csv']\n",
    "# OneVsAllDataSetName = 'basin1Light_Clean.csv'\n",
    "\n",
    "for datasetForTest in allDataSetsNames:\n",
    "    filename, file_extension = os.path.splitext(datasetForTest)\n",
    "    newListOfNames = [s for s in allDataSetsNames if s != datasetForTest]\n",
    "    allDataSetsFileName = 'allVs_'+ filename +'_Training'\n",
    "    DFToConcatAll = pd.DataFrame()\n",
    "#     DFToConcatAll = pd.read_csv((sourceFileForDatasets+datasetForTest), index_col = None)\n",
    "#     print(DFToConcatAll.head())\n",
    "    for datasets in newListOfNames:\n",
    "        DFToConcatAll = pd.concat([DFToConcatAll, pd.read_csv((sourceFileForDatasets+datasets), index_col = None)])\n",
    "    nameToSafe = sourceFileForDatasets+allDataSetsFileName+file_extension\n",
    "    DFToConcatAll.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "    DFToConcatAll.to_csv(nameToSafe, index=None)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destiationPath = 'datasets/RFdatasets'\n",
    "listFile = os.listdir(destiationPath)\n",
    "print(listFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testList = ['basin1Light_Clean_Test.csv','basin2_Test.csv','basin3_Test.csv','basin4_Test.csv','basin5_Test.csv']\n",
    "traininList = ['basin1Light_Clean_Training.csv','basin2_Training.csv','basin3_Training.csv','basin4_Training.csv','basin5_Training.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning datasets: Removing not usefull variables from All_VS_ONE \n",
    "readPath = 'datasets/RFdatasets/'\n",
    "destiationPath = 'datasets/dataset4MLP/'\n",
    "datasetNamelist = ['basin1Light_Clean_Test.csv','basin2_Test.csv','basin3_Test.csv','basin4_Test.csv','basin5_Test.csv',\n",
    "                  'basin1Light_Clean_Training.csv','basin2_Training.csv','basin3_Training.csv','basin4_Training.csv',\n",
    "                   'basin5_Training.csv']\n",
    "featuresToDelete = ['TPI','TWI']\n",
    "for i in datasetNamelist:\n",
    "    path = readPath + i \n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    basinDataSet.drop(featuresToDelete, axis=1, inplace=True)\n",
    "    savePath = destiationPath + 'MLP_'+ i\n",
    "    basinDataSet.to_csv(savePath, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###. Build dataset subset for MLP test (Only first 150K samples)\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "datasetNamelist = ['MLP_allVs_basin1Light_Clean_Training.csv','MLP_basin1Light_Clean_VsAll_Test.csv']\n",
    "for i in datasetNamelist:\n",
    "    path = readPath + i \n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    Y = np.array(basinDataSet['percentage'])\n",
    "    count,_ = md.listClassCountPercent(Y)\n",
    "    basinDataSet.drop(basinDataSet.loc[150000:count].index,axis=0,inplace=True)\n",
    "    savePath = readPath + 'reduced_'+i\n",
    "    basinDataSet.to_csv(savePath, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring datasets\n",
    "dataset = ['reduced_MLP_allVs_basin1Light_Clean_Training.csv','reduced_MLP_basin1Light_Clean_VsAll_Test.csv']\n",
    "for i in dataset:\n",
    "    path = readPath + i \n",
    "    print(path)\n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    print(basinDataSet.head())\n",
    "    Y = np.array(basinDataSet['percentage'])\n",
    "    print(md.listClassCountPercent(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.  Training TEST\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "trainingPath = readPath + 'MLP_allVs_basin1Light_Clean_Training.csv'\n",
    "params = {'random_state':50, 'hidden_layer_sizes': 2,\n",
    "                'early_stopping':True,'max_iter':200,'verbose':False,\n",
    "                'tol':0.00010,'validation_fraction':0.1,'warm_start':False}\n",
    "mlpc = md.implementingMLPCalssifier(trainingPath,'percentage',params)\n",
    "mlpc.fitMLPClassifier()\n",
    "mlpc.plotLossBehaviour()\n",
    "\n",
    "mlpClassifier = mlpc.getMLPClassifier()\n",
    "\n",
    "# #Validating un unseen datase\n",
    "# validation = readPath + 'MLP_basin1Light_Clean_VsAll_Test.csv'\n",
    "# x_val,y_val = ms.importDataSet(validation, 'percentage')\n",
    "# prediction = ms.makePredictionToImportAsSHP(mlpClassifier, x_val, y_val, 'percentage')\n",
    "\n",
    "# ## Compute metrics\n",
    "# X = x_val.copy()\n",
    "# X.drop(['x_coord','y_coord'], axis=1, inplace=True)\n",
    "# metrics = md.computeClassificationMetrics(mlpClassifier,X,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlpc.get_logsDic())\n",
    "mlpc.logMLPClassifier({'test':34})\n",
    "print(mlpc.get_logsDic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(('outputs/'+ 'MLP_basin1Light_firstResult_HL280.csv'),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement bets hiddenLayerSize exploration\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "trainingPath = readPath + 'MLP_allVs_basin1Light_Clean_Training.csv'\n",
    "validation = readPath + 'MLP_basin1Light_Clean_VsAll_Test.csv'\n",
    "# dataset = pd.read_csv(trainingPath, index_col = None)\n",
    "params = {'random_state':50, 'hidden_layer_sizes': 2,\n",
    "                'early_stopping':False,'max_iter':2,'verbose':True,\n",
    "                'tol':0.00010,'validation_fraction':0.1,'warm_start':False}\n",
    "mlpc = md.implementingMLPCalssifier(trainingPath,'percentage',params)\n",
    "\n",
    "x_val,Y_val = ms.importDataSet(validation, 'percentage')\n",
    "X = x_val.copy()\n",
    "X.drop(['x_coord','y_coord'], axis=1, inplace=True)\n",
    "firstInterval = np.arange(100,1009,100)\n",
    "mlpc.explore4BestHLSize(X,Y_val,firstInterval,'5',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controled sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = pd.read_csv('datasets/basin2 _Training.csv', index_col = None)\n",
    "print(DS.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(DS['FAProx_01']) # , , DS['elevation'], DS['disToRiv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling appliying class selection by rule:\n",
    "\n",
    "# RULE1: Select point at a distance to river less than 300m. \n",
    "\n",
    "# # newDS = pseudoClassCreation(DS, \"distanceToRiver\", 300, 2)\n",
    "def pseudoClassCreation(dataset, conditionVariable, threshold, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Replace <targetClass> by  <pseudoClass> where <conditionVariable >= threshold>. \n",
    "    Return:\n",
    "      dataset with new classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    conditionVar = (np.array(dataset[conditionVariable])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ pseudoClass if conditionVar[j] >= threshold \n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "def revertPseudoClassCreation(dataset, originalClass, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Restablich  <targetClass> with <originalClass> where <targetClassName == pseudoClass>. \n",
    "    Return:\n",
    "      dataset with original classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ originalClass if actualTarget[j] == pseudoClass\n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "\n",
    "print(Counter(X_train['percentage']))\n",
    "newDS = pseudoClassCreation(X_train, 'disToRiv', 200, 2, 'percentage')\n",
    "y = newDS['percentage']\n",
    "newDS.drop(['percentage'], axis=1, inplace = True)\n",
    "x_res,y_res = ms.randomUndersampling(newDS, y, )\n",
    "x_res['percentage'] = y_res\n",
    "# newDatase = revertPseudoClassCreation(x_res, 0, 2, 'percentage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res.to_csv('basin1ControlClass0Sampling4Class_ToSHP.csv',index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import dataset to describe\n",
    "DS= pd.read_csv('datasets/basin4_Training.csv', index_col=None)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.drop(['x_coord','y_coord'], axis = 1, inplace=True)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAcc vs Labels\n",
    "targets = DS['percentage']\n",
    "FAcc = original['FAcc']\n",
    "FAcc_norm = DS['FAcc_norm']\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13,4), sharey=True)\n",
    "fig.text(-0.02, 0.5, 'labels', va='center', rotation='vertical')\n",
    "fig.text(0.5, 1, 'Flow accumulation vs labels distribution', ha ='center')\n",
    "axs[0].scatter(FAcc,targets)\n",
    "# axs[0].set_title(\"Facc\")\n",
    "axs[0].set(xlabel='a) Flow Accumulation')\n",
    "axs[1].scatter(FAcc_norm,targets)\n",
    "# axs[1].set_title(\"FAcc_norm\")\n",
    "axs[1].set(xlabel='b) Flow Accumulation estandardized')\n",
    "plt.rcParams['font.size'] = '20'\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Plot all features vs labels\n",
    "# 'disToRiv', 'TWI', 'TPI', 'slope', 'elevation',\n",
    "\n",
    "targets = DS['percentage']\n",
    "# targets = np.where(targets == 5,2,targets)\n",
    "\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['LDSOL4R150']\n",
    "DLSOL5R150 = DS['LDSOL5R150']\n",
    "DLSOL5R200 = DS['LDSOL5R200']\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "\n",
    "fig, axs = plt.subplots(4,3, figsize=(13, 8), sharey=True)\n",
    "fig.supylabel('Labels')\n",
    "plt.rcParams['font.size'] = '15'\n",
    "plt.yticks([0,1,5])\n",
    "\n",
    "'''\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "'''\n",
    "axs[0, 0].scatter(E,targets)\n",
    "axs[0, 0].set_title(\"Elevation\")\n",
    "axs[1, 0].scatter(slope,targets)\n",
    "axs[1, 0].set_title(\"Slope\")\n",
    "axs[2, 0].scatter(FAcc,targets)\n",
    "axs[2, 0].set_title(\"Flow accumulation\")\n",
    "axs[3, 0].scatter(TWI,targets)\n",
    "axs[3, 0].set_title(\"TWI\")\n",
    "\n",
    "'''\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['DLSOL4R150']\n",
    "DLSOL5R150 = DS['DLSOL5R150']\n",
    "DLSOL5R200 = DS['DLSOL5R200']\n",
    "'''\n",
    "axs[0, 1].scatter(TPI,targets)\n",
    "axs[0, 1].set_title('TPI')\n",
    "axs[1, 1].scatter(DLSOL4R150,targets)\n",
    "axs[1, 1].set_title(\"DLSOL4R150\")\n",
    "axs[2, 1].scatter(DLSOL5R150,targets)\n",
    "axs[2, 1].set_title(\"DLSOL5R150\")\n",
    "axs[3, 1].scatter(DLSOL5R200,targets)\n",
    "axs[3, 1].set_title(\"DLSOL5R200\")\n",
    "\n",
    "'''\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "'''\n",
    "axs[0, 2].scatter(FAProx_01,targets)\n",
    "axs[0, 2].set_title('FAProx_01')\n",
    "axs[1, 2].scatter(FAProx_025,targets)\n",
    "axs[1, 2].set_title(\"FAProx_025\")\n",
    "axs[2, 2].scatter(visibility,targets)\n",
    "axs[2, 2].set_title(\"Visibility\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.head())\n",
    "#  Return a dataset with the rows corresponding to the index where condition in DS.columName is valid. \n",
    "dsArray = DS[DS.percentage != 0] \n",
    "print(dsArray.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.pairplot(DS, hue = 'percentage', diag_kind = 'kde', \n",
    "             plot_kws = {'alpha': 0.8, 's': 100},\n",
    "             height = 4, corner=True, palette = \"Set2\")# vars = ['life_exp', 'log_pop', 'log_gdp_per_cap'],\n",
    "\n",
    "# sns.pairplot(DS, hue=\"percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####. Covariance Matrix\n",
    "sns.set(font_scale=0.7)\n",
    "matrix = DS.corr().round(2)\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.set_figsize=(25,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(criterion='entropy', random_state = 50)\n",
    "x_train,y_train = ms.importDataSet('basin1Train.csv', 'percentage')\n",
    "classifier = OneVsRestClassifier(estimator).fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ms.loadModel('./outputs/2022-08-05/11-01-57/2208051101.pkl')\n",
    "x_test,y_test = ms.importDataSet('basin1Test.csv', 'percentage')\n",
    "\n",
    "x_test = ms.removeCoordinatesFromDataSet(x_test)\n",
    "\n",
    "# y_prob = classifier.predict_proba(x_test)\n",
    "#print(np.unique(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.plot_ROC_AUC_OneVsRest(classifier, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,y_test = ms.importDataSet('./bestModels/Classifier/10-18-08/2208051018prediction_basin1Test.csv', 'prediction')\n",
    "unique, count = np.unique(y_test, return_counts=True)\n",
    "total = count.sum()\n",
    "print(total)\n",
    "percent = np.round(np.zeros_like(unique).astype('float16'),3)\n",
    "print('values, counts , percent')\n",
    "for i in range(len(unique)):    \n",
    "   percent[i] = (count[i]/total)*100\n",
    "   print(unique[i],\"\\t\", count[i], percent[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhiteBoxTool applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whiteboxApplications as wbtapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = '/Users/abdielfer/DESS/Internship2022/RNCanWork/FloodMaps/WBTWorkDir'\n",
    "rtools = wbtapp.rasterTools(wdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    rtools.gaussianFilter('basin1_FAccDInfLogGauss2.tif', 'basin1_FAccDInfLogGauss2.tif', sigma = 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c444c068df4da933f83035bc75b2ed5aea8e7fb8bc113357e2f14ee7cca6a8fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
