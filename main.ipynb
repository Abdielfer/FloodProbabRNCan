{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import myServices as ms\n",
    "import models as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_curve, auc, roc_auc_score, f1_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute ececution time do: \n",
    "# with timeit():\n",
    "#     # your code, e.g., \n",
    "class timeit(): \n",
    "    from datetime import datetime\n",
    "    def __enter__(self):\n",
    "        self.tic = self.datetime.now()\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print('runtime: {}'.format(self.datetime.now() - self.tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and manipulating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3504734 entries, 0 to 3504733\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   percentage  int64  \n",
      " 1   TWI         float64\n",
      " 2   TPI         float64\n",
      " 3   DLSOL4R150  float64\n",
      " 4   DLSOL5R150  float64\n",
      " 5   DLSOL5R200  float64\n",
      " 6   FAProx_01   float64\n",
      " 7   FAProx_025  float64\n",
      " 8   FAcc        float64\n",
      " 9   visibility  float64\n",
      " 10  slope       float64\n",
      " 11  elevation   float64\n",
      " 12  x_coord     float64\n",
      " 13  y_coord     float64\n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 374.3 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>TWI</th>\n",
       "      <th>TPI</th>\n",
       "      <th>DLSOL4R150</th>\n",
       "      <th>DLSOL5R150</th>\n",
       "      <th>DLSOL5R200</th>\n",
       "      <th>FAProx_01</th>\n",
       "      <th>FAProx_025</th>\n",
       "      <th>FAcc</th>\n",
       "      <th>visibility</th>\n",
       "      <th>slope</th>\n",
       "      <th>elevation</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "      <td>3.504734e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.168953e-01</td>\n",
       "      <td>8.050545e+00</td>\n",
       "      <td>-3.719054e-04</td>\n",
       "      <td>1.482946e-03</td>\n",
       "      <td>3.391483e-03</td>\n",
       "      <td>3.387184e-03</td>\n",
       "      <td>1.383523e+02</td>\n",
       "      <td>1.757246e+02</td>\n",
       "      <td>1.163574e-03</td>\n",
       "      <td>7.023017e-02</td>\n",
       "      <td>5.448437e+00</td>\n",
       "      <td>1.378321e+02</td>\n",
       "      <td>3.635782e+05</td>\n",
       "      <td>5.268173e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.424222e-01</td>\n",
       "      <td>5.030281e+00</td>\n",
       "      <td>2.016044e-01</td>\n",
       "      <td>2.407676e-03</td>\n",
       "      <td>3.662252e-03</td>\n",
       "      <td>3.200014e-03</td>\n",
       "      <td>1.540761e+02</td>\n",
       "      <td>1.796489e+02</td>\n",
       "      <td>2.250805e-02</td>\n",
       "      <td>6.390308e-02</td>\n",
       "      <td>5.371838e+00</td>\n",
       "      <td>4.009163e+01</td>\n",
       "      <td>3.205929e+03</td>\n",
       "      <td>5.377868e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.386545e+01</td>\n",
       "      <td>-4.857010e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>3.900000e-04</td>\n",
       "      <td>-3.685000e-02</td>\n",
       "      <td>3.582080e+05</td>\n",
       "      <td>5.260383e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.050610e+00</td>\n",
       "      <td>-7.068000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.700000e-04</td>\n",
       "      <td>3.471311e+01</td>\n",
       "      <td>1.081665e+01</td>\n",
       "      <td>1.100086e-06</td>\n",
       "      <td>2.277000e-02</td>\n",
       "      <td>1.815492e+00</td>\n",
       "      <td>1.154975e+02</td>\n",
       "      <td>3.610830e+05</td>\n",
       "      <td>5.263883e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.887130e+00</td>\n",
       "      <td>-4.600000e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.650000e-03</td>\n",
       "      <td>2.960000e-03</td>\n",
       "      <td>8.044874e+01</td>\n",
       "      <td>1.154340e+02</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>5.093000e-02</td>\n",
       "      <td>3.785225e+00</td>\n",
       "      <td>1.412771e+02</td>\n",
       "      <td>3.628680e+05</td>\n",
       "      <td>5.266133e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.222318e+01</td>\n",
       "      <td>6.900000e-02</td>\n",
       "      <td>2.950000e-03</td>\n",
       "      <td>5.380000e-03</td>\n",
       "      <td>5.310000e-03</td>\n",
       "      <td>1.790475e+02</td>\n",
       "      <td>3.047950e+02</td>\n",
       "      <td>4.000000e-05</td>\n",
       "      <td>9.773000e-02</td>\n",
       "      <td>7.296218e+00</td>\n",
       "      <td>1.637871e+02</td>\n",
       "      <td>3.659630e+05</td>\n",
       "      <td>5.272993e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.486353e+01</td>\n",
       "      <td>4.880940e+00</td>\n",
       "      <td>1.693000e-02</td>\n",
       "      <td>2.363000e-02</td>\n",
       "      <td>1.903000e-02</td>\n",
       "      <td>9.000000e+02</td>\n",
       "      <td>9.751410e+02</td>\n",
       "      <td>1.009800e+00</td>\n",
       "      <td>4.991200e-01</td>\n",
       "      <td>6.760703e+01</td>\n",
       "      <td>2.546740e+02</td>\n",
       "      <td>3.719280e+05</td>\n",
       "      <td>5.279338e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         percentage           TWI           TPI    DLSOL4R150    DLSOL5R150  \\\n",
       "count  3.504734e+06  3.504734e+06  3.504734e+06  3.504734e+06  3.504734e+06   \n",
       "mean   1.168953e-01  8.050545e+00 -3.719054e-04  1.482946e-03  3.391483e-03   \n",
       "std    7.424222e-01  5.030281e+00  2.016044e-01  2.407676e-03  3.662252e-03   \n",
       "min    0.000000e+00 -1.386545e+01 -4.857010e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  5.050610e+00 -7.068000e-02  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  7.887130e+00 -4.600000e-04  0.000000e+00  2.650000e-03   \n",
       "75%    0.000000e+00  1.222318e+01  6.900000e-02  2.950000e-03  5.380000e-03   \n",
       "max    5.000000e+00  2.486353e+01  4.880940e+00  1.693000e-02  2.363000e-02   \n",
       "\n",
       "         DLSOL5R200     FAProx_01    FAProx_025          FAcc    visibility  \\\n",
       "count  3.504734e+06  3.504734e+06  3.504734e+06  3.504734e+06  3.504734e+06   \n",
       "mean   3.387184e-03  1.383523e+02  1.757246e+02  1.163574e-03  7.023017e-02   \n",
       "std    3.200014e-03  1.540761e+02  1.796489e+02  2.250805e-02  6.390308e-02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e-05   \n",
       "25%    1.700000e-04  3.471311e+01  1.081665e+01  1.100086e-06  2.277000e-02   \n",
       "50%    2.960000e-03  8.044874e+01  1.154340e+02  1.000000e-05  5.093000e-02   \n",
       "75%    5.310000e-03  1.790475e+02  3.047950e+02  4.000000e-05  9.773000e-02   \n",
       "max    1.903000e-02  9.000000e+02  9.751410e+02  1.009800e+00  4.991200e-01   \n",
       "\n",
       "              slope     elevation       x_coord       y_coord  \n",
       "count  3.504734e+06  3.504734e+06  3.504734e+06  3.504734e+06  \n",
       "mean   5.448437e+00  1.378321e+02  3.635782e+05  5.268173e+06  \n",
       "std    5.371838e+00  4.009163e+01  3.205929e+03  5.377868e+03  \n",
       "min    3.900000e-04 -3.685000e-02  3.582080e+05  5.260383e+06  \n",
       "25%    1.815492e+00  1.154975e+02  3.610830e+05  5.263883e+06  \n",
       "50%    3.785225e+00  1.412771e+02  3.628680e+05  5.266133e+06  \n",
       "75%    7.296218e+00  1.637871e+02  3.659630e+05  5.272993e+06  \n",
       "max    6.760703e+01  2.546740e+02  3.719280e+05  5.279338e+06  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Cleaning basin1DataSet \n",
    "dataSetPath = 'datasets/allVs_basin2_CleanDataSet_Training.csv'\n",
    "basinDataSet = pd.read_csv(dataSetPath, index_col = None)\n",
    "# basin1Light = pd.read_csv('datasetBasin1_NoDataFree.csv', index_col = None)\n",
    "print(basinDataSet.info())\n",
    "basinDataSet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentage    False\n",
       "TWI           False\n",
       "TPI           False\n",
       "DLSOL4R150    False\n",
       "DLSOL5R150    False\n",
       "DLSOL5R200    False\n",
       "FAProx_01     False\n",
       "FAProx_025    False\n",
       "FAcc          False\n",
       "visibility    False\n",
       "slope         False\n",
       "elevation     False\n",
       "x_coord       False\n",
       "y_coord       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basinDataSet.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['percentage','DLSOL5R200', 'DLSOL4R150', 'DLSOL5R150']\n",
    "for col in colNames: \n",
    "    basinDataSet[col].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.dropna(subset=['TPI'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.drop(['fid'], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentage    0\n",
       "TWI           0\n",
       "TPI           0\n",
       "DLSOL4R150    0\n",
       "DLSOL5R150    0\n",
       "DLSOL5R200    0\n",
       "FAProx_01     0\n",
       "FAProx_025    0\n",
       "FAcc          0\n",
       "visibility    0\n",
       "slope         0\n",
       "elevation     0\n",
       "x_coord       0\n",
       "y_coord       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basinDataSet.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOrmalize Flow Accumulation\n",
    "basinDataSet['FAcc'] = (basinDataSet['FAcc']- basinDataSet['FAcc'].min())/(basinDataSet['FAcc'].max()-basinDataSet['FAcc'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing QGIS NoData value(-9999) with 0 \n",
    "repalcer  = basinDataSet['FAProx_01'].to_numpy()\n",
    "basinDataSet['FAProx_01'] = [0 if repalcer[j] == -9999 else repalcer[j] for j in range(len(repalcer))]                                                                                                                         \n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform a column datatype\n",
    "repalcer  = basinDataSet['percentage'].to_numpy().astype('int')\n",
    "basinDataSet.loc[:,'percentage'] = repalcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.to_csv('datasets/basin2_CleanDataSet.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS.head(5)\n",
    "s = {}\n",
    "s['Datas'] = ds\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportional Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X,Y = ms.importDataSet('datasets/basin2_CleanDataSet.csv', 'percentage')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=50)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    print(\"TRAIN:\", train_index.size, \"TEST:\", test_index.size)\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = Y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = Y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describing training set\n",
    "print(len(X_train['elevation']), len(y_train) )\n",
    "trainCount = Counter(y_train)\n",
    "print(trainCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####    Creating training set     #####\n",
    "X_train.loc[:,'percentage'] = y_train\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing coordinates from training set\n",
    "X_train.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('datasets/basin2 _Training.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####. Creating Test set\n",
    "print(X_test.head())\n",
    "X_test.loc[:,'percentage'] = y_test\n",
    "print(X_test.head())\n",
    "print(X_test.info())\n",
    "testCount = Counter(X_test['percentage'])\n",
    "print(f\"testCount:  {testCount}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('datasets/basin2_Test.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This proportions are the reason why a sample_weight of 0.01 for the majority class give best results for regression\n",
    "totalTrain = sum([trainCount[0], trainCount[1], trainCount[5]]) \n",
    "totalValidation = sum([testCount[0], testCount[1], testCount[5]])\n",
    "print(f\"total Train samples: {totalTrain},  total Validation samples: {totalValidation}\")\n",
    "print(\"Summary of traning and test dataset class balance\")\n",
    "print(f\"Training Set:\", '\\n', \"Class 0: %.3f\" %(trainCount[0]/totalTrain), \" Class 1: %.4f\" %(trainCount[1]/totalTrain), \"Class 5: %.4f\"%(trainCount[5]/totalTrain))\n",
    "print(\"Testing Set:\", '\\n', \"Class 0: %.3f\" %(testCount[0]/totalValidation),\" Class 1: %.4f\" %(testCount[1]/totalValidation),  \"Class 5: %.4f\"%(testCount[5]/totalValidation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms.loadModel('./outputs/2022-08-05/00-35-58/2208050035.pkl')\n",
    "dataSetToSave = ms.makePredictionToImportAsSHP(csvName, model, X, Y, 'percentage')\n",
    "print(dataSetToSave.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining dataSets to build AllVsOne_training and OneVsAll_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat datasets\n",
    "sourceFileForDatasets = 'datasets/'\n",
    "allDataSetsNames = ['basin1Light_Clean.csv', 'basin2_CleanDataSet.csv', 'basin3_CleanDataSet_copy.csv','basin4_CleanDataSet.csv','basin5_CleanDataSet.csv']\n",
    "# OneVsAllDataSetName = 'basin1Light_Clean.csv'\n",
    "\n",
    "for datasetForTest in allDataSetsNames:\n",
    "    filename, file_extension = os.path.splitext(datasetForTest)\n",
    "    newListOfNames = [s for s in allDataSetsNames if s != datasetForTest]\n",
    "    allDataSetsFileName = 'allVs_'+ filename +'_Training'\n",
    "#     allDataSetsFileName = pd.DataFrame()\n",
    "    DFToConcatAll = pd.read_csv((sourceFileForDatasets+datasetForTest), index_col = None)\n",
    "#     print(DFToConcatAll.head())\n",
    "    for datasets in newListOfNames:\n",
    "        DFToConcatAll = pd.concat([DFToConcatAll, pd.read_csv((sourceFileForDatasets+datasets), index_col = None)])\n",
    "    nameToSafe = sourceFileForDatasets+allDataSetsFileName+file_extension\n",
    "    DFToConcatAll.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "    DFToConcatAll.to_csv(nameToSafe, index=None)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controled sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = pd.read_csv('datasets/basin2 _Training.csv', index_col = None)\n",
    "print(DS.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(DS['FAProx_01']) # , , DS['elevation'], DS['disToRiv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling appliying class selection by rule:\n",
    "\n",
    "# RULE1: Select point at a distance to river less than 300m. \n",
    "\n",
    "# # newDS = pseudoClassCreation(DS, \"distanceToRiver\", 300, 2)\n",
    "def pseudoClassCreation(dataset, conditionVariable, threshold, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Replace <targetClass> by  <pseudoClass> where <conditionVariable >= threshold>. \n",
    "    Return:\n",
    "      dataset with new classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    conditionVar = (np.array(dataset[conditionVariable])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ pseudoClass if conditionVar[j] >= threshold \n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "def revertPseudoClassCreation(dataset, originalClass, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Restablich  <targetClass> with <originalClass> where <targetClassName == pseudoClass>. \n",
    "    Return:\n",
    "      dataset with original classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ originalClass if actualTarget[j] == pseudoClass\n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "\n",
    "print(Counter(X_train['percentage']))\n",
    "newDS = pseudoClassCreation(X_train, 'disToRiv', 200, 2, 'percentage')\n",
    "y = newDS['percentage']\n",
    "newDS.drop(['percentage'], axis=1, inplace = True)\n",
    "x_res,y_res = ms.randomUndersampling(newDS, y, )\n",
    "x_res['percentage'] = y_res\n",
    "# newDatase = revertPseudoClassCreation(x_res, 0, 2, 'percentage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res.to_csv('basin1ControlClass0Sampling4Class_ToSHP.csv',index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import dataset to describe\n",
    "DS= pd.read_csv('datasets/basin4_Training.csv', index_col=None)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.drop(['x_coord','y_coord'], axis = 1, inplace=True)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAcc vs Labels\n",
    "targets = DS['percentage']\n",
    "FAcc = original['FAcc']\n",
    "FAcc_norm = DS['FAcc_norm']\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13,4), sharey=True)\n",
    "fig.text(-0.02, 0.5, 'labels', va='center', rotation='vertical')\n",
    "fig.text(0.5, 1, 'Flow accumulation vs labels distribution', ha ='center')\n",
    "axs[0].scatter(FAcc,targets)\n",
    "# axs[0].set_title(\"Facc\")\n",
    "axs[0].set(xlabel='a) Flow Accumulation')\n",
    "axs[1].scatter(FAcc_norm,targets)\n",
    "# axs[1].set_title(\"FAcc_norm\")\n",
    "axs[1].set(xlabel='b) Flow Accumulation estandardized')\n",
    "plt.rcParams['font.size'] = '20'\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Plot all features vs labels\n",
    "# 'disToRiv', 'TWI', 'TPI', 'slope', 'elevation',\n",
    "\n",
    "targets = DS['percentage']\n",
    "# targets = np.where(targets == 5,2,targets)\n",
    "\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['LDSOL4R150']\n",
    "DLSOL5R150 = DS['LDSOL5R150']\n",
    "DLSOL5R200 = DS['LDSOL5R200']\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "\n",
    "fig, axs = plt.subplots(4,3, figsize=(13, 8), sharey=True)\n",
    "fig.supylabel('Labels')\n",
    "plt.rcParams['font.size'] = '15'\n",
    "plt.yticks([0,1,5])\n",
    "\n",
    "'''\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "'''\n",
    "axs[0, 0].scatter(E,targets)\n",
    "axs[0, 0].set_title(\"Elevation\")\n",
    "axs[1, 0].scatter(slope,targets)\n",
    "axs[1, 0].set_title(\"Slope\")\n",
    "axs[2, 0].scatter(FAcc,targets)\n",
    "axs[2, 0].set_title(\"Flow accumulation\")\n",
    "axs[3, 0].scatter(TWI,targets)\n",
    "axs[3, 0].set_title(\"TWI\")\n",
    "\n",
    "'''\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['DLSOL4R150']\n",
    "DLSOL5R150 = DS['DLSOL5R150']\n",
    "DLSOL5R200 = DS['DLSOL5R200']\n",
    "'''\n",
    "axs[0, 1].scatter(TPI,targets)\n",
    "axs[0, 1].set_title('TPI')\n",
    "axs[1, 1].scatter(DLSOL4R150,targets)\n",
    "axs[1, 1].set_title(\"DLSOL4R150\")\n",
    "axs[2, 1].scatter(DLSOL5R150,targets)\n",
    "axs[2, 1].set_title(\"DLSOL5R150\")\n",
    "axs[3, 1].scatter(DLSOL5R200,targets)\n",
    "axs[3, 1].set_title(\"DLSOL5R200\")\n",
    "\n",
    "'''\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "'''\n",
    "axs[0, 2].scatter(FAProx_01,targets)\n",
    "axs[0, 2].set_title('FAProx_01')\n",
    "axs[1, 2].scatter(FAProx_025,targets)\n",
    "axs[1, 2].set_title(\"FAProx_025\")\n",
    "axs[2, 2].scatter(visibility,targets)\n",
    "axs[2, 2].set_title(\"Visibility\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.head())\n",
    "#  Return a dataset with the rows corresponding to the index where condition in DS.columName is valid. \n",
    "dsArray = DS[DS.percentage != 0] \n",
    "print(dsArray.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.pairplot(DS, hue = 'percentage', diag_kind = 'kde', \n",
    "             plot_kws = {'alpha': 0.8, 's': 100},\n",
    "             height = 4, corner=True, palette = \"Set2\")# vars = ['life_exp', 'log_pop', 'log_gdp_per_cap'],\n",
    "\n",
    "# sns.pairplot(DS, hue=\"percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####. Covariance Matrix\n",
    "sns.set(font_scale=0.7)\n",
    "matrix = DS.corr().round(2)\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.set_figsize=(25,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(criterion='entropy', random_state = 50)\n",
    "x_train,y_train = ms.importDataSet('basin1Train.csv', 'percentage')\n",
    "classifier = OneVsRestClassifier(estimator).fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ms.loadModel('./outputs/2022-08-05/11-01-57/2208051101.pkl')\n",
    "x_test,y_test = ms.importDataSet('basin1Test.csv', 'percentage')\n",
    "\n",
    "x_test = ms.removeCoordinatesFromDataSet(x_test)\n",
    "\n",
    "# y_prob = classifier.predict_proba(x_test)\n",
    "#print(np.unique(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.plot_ROC_AUC_OneVsRest(classifier, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,y_test = ms.importDataSet('./bestModels/Classifier/10-18-08/2208051018prediction_basin1Test.csv', 'prediction')\n",
    "unique, count = np.unique(y_test, return_counts=True)\n",
    "total = count.sum()\n",
    "print(total)\n",
    "percent = np.round(np.zeros_like(unique).astype('float16'),3)\n",
    "print('values, counts , percent')\n",
    "for i in range(len(unique)):    \n",
    "   percent[i] = (count[i]/total)*100\n",
    "   print(unique[i],\"\\t\", count[i], percent[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c444c068df4da933f83035bc75b2ed5aea8e7fb8bc113357e2f14ee7cca6a8fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
