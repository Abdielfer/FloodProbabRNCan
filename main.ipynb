{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import myServices as ms\n",
    "import models as md\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_curve, auc, roc_auc_score, f1_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute ececution time do: \n",
    "# with timeit():\n",
    "#     # your code, e.g., \n",
    "class timeit(): \n",
    "    from datetime import datetime\n",
    "    def __enter__(self):\n",
    "        self.tic = self.datetime.now()\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print('runtime: {}'.format(self.datetime.now() - self.tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explore model capacity vs performance on unseening basin. ####\n",
    "\n",
    "'''\n",
    "hidden_layer_sizes=[100,100] earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6228742547616204, 1: 0.4999938829459356, 5: 0.6399651931990683}\n",
    "_____\n",
    "hidden_layer_sizes=[100,100] earlyStop False, Epochs = 45  ITÂ´s NOT Better!!\n",
    "_____\n",
    "hidden_layer_sizes=[1000,1000] earlyStop True: \n",
    "ROC_AUC one_vs_all:  {0: 0.6125170698989215, 1: 0.49991965918329867, 5: 0.6254008937160408}\n",
    "_____\n",
    "hidden_layer_sizes=(150,100,50), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.5637879768798603, 1: 0.499983295736978, 5: 0.5729145574443686}\n",
    "_____\n",
    "hidden_layer_sizes=(200,150,100,50), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6308461923938076, 1: 0.5000229432884631, 5: 0.6420998385622048}\n",
    "______\n",
    "hidden_layer_sizes=(200), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.654696371004719, 1: 0.5, 5: 0.6698573647187008}\n",
    "______\n",
    "hidden_layer_sizes=(278), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6739319215706956, 1: 0.5, 5: 0.687665244202922}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15007a490>, \n",
    "'activation': 'relu', 'n_iter': 42}\n",
    "_____\n",
    "ROC_AUC one_vs_all:  {0: 0.6434244793232012, 1: 0.4999990589147593, 5: 0.6580651931595939}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x14ffd44f0>, \n",
    "'activation': 'relu', 'hidden_layer_sizes': 279, 'n_iter': 26}\n",
    "______\n",
    "###.  BEST  #####\n",
    "'hidden_layer_sizes': 280\n",
    "ROC_AUC one_vs_all:  {0: 0.725413336714216, 1: 0.5, 5: 0.739561606898303}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x11115f880>, \n",
    "'activation': 'relu', 'n_iter': 32}\n",
    "____\n",
    "'hidden_layer_sizes': 281\n",
    "ROC_AUC one_vs_all:  {0: 0.687069575713219, 1: 0.5, 5: 0.7018609166432868}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150412280>, \n",
    "'activation': 'relu', 'n_iter': 32}\n",
    "_____\n",
    "'hidden_layer_sizes': 282\n",
    "ROC_AUC one_vs_all:  {0: 0.7091773125462395, 1: 0.5025620860825134, 5: 0.7231809060694995}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x14ffd43a0>, \n",
    "'activation': 'relu', 'hidden_layer_sizes': 282, 'n_iter': 32}\n",
    "______\n",
    "hidden_layer_sizes=(300), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6712874739483707, 1: 0.5, 5: 0.6903606434727337}\n",
    "_____\n",
    "hidden_layer_sizes=(350), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6634314979751396, 1: 0.5, 5: 0.6743297808926096}\n",
    "_____\n",
    "hidden_layer_sizes=(400), earlyStop True\n",
    "ROC_AUC one_vs_all:  {0: 0.6735994541470123, 1: 0.5, 5: 0.6886358464246944}\n",
    "____\n",
    "hidden_layer_sizes = 500\n",
    "ROC_AUC one_vs_all:  {0: 0.6413823150589926, 1: 0.5, 5: 0.6570811552402528}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15057a070>, \n",
    "'activation': 'relu', 'n_iter': 27}\n",
    "____\n",
    "hidden_layer_sizes = 600, \n",
    "ROC_AUC one_vs_all:  {0: 0.6629380284306443, 1: 0.5, 5: 0.678143405544148}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150143cd0>, \n",
    "'activation': 'relu', 'n_iter': 29}\n",
    "____\n",
    "hidden_layer_sizes' = 690\n",
    "ROC_AUC one_vs_all:  {0: 0.6720056649260504, 1: 0.5, 5: 0.684244579416157}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150528610>, \n",
    "'activation': 'relu', 'n_iter': 27}\n",
    "____\n",
    "###. to check####\n",
    "hidden_layer_sizes = 700\n",
    "ROC_AUC one_vs_all:  {0: 0.709241541871279, 1: 0.5, 5: 0.7225650719554007}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1501435e0>, \n",
    "'activation': 'relu', 'n_iter': 34}\n",
    "____\n",
    "hidden_layer_sizes = 710\n",
    "ROC_AUC one_vs_all:  {0: 0.6748179832958519, 1: 0.5, 5: 0.6911761628358285}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1500fea30>, '\n",
    "activation': 'relu', 'n_iter': 40}\n",
    "____\n",
    "hidden_layer_sizes = 800,\n",
    "ROC_AUC one_vs_all:  {0: 0.6665697027100255, 1: 0.5, 5: 0.6828522971609348}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x150145f70>,\n",
    "'activation': 'relu', 'n_iter': 34}\n",
    "____\n",
    "hidden_layer_sizes = 900\n",
    "ROC_AUC one_vs_all:  {0: 0.6361267478738225, 1: 0.5, 5: 0.6481332615660654}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x1500fe460>, \n",
    "'activation': 'relu','n_iter': 33}\n",
    "_____\n",
    "hidden_layer_sizes = 1000\n",
    "ROC_AUC one_vs_all:  {0: 0.6714374119850677, 1: 0.5, 5: 0.6856021298501717}\n",
    "{'optimizer': <sklearn.neural_network._stochastic_optimizers.AdamOptimizer object at 0x15007a640>, \n",
    "'activation': 'relu', 'n_iter': 36}\n",
    "'''  \n",
    "prediction.to_csv(('outputs/'+ 'MLP_basin1Light_firstResult_HL280.csv'),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and manipulating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import DataSet \n",
    "readSetPath = 'datasets/dataset4MLP/'\n",
    "importName = 'MLP_basin2_CleanDataSet_VsAll_Test.csv'\n",
    "saveDatasetPath = 'datasets/datasets4MLP_Binary/'\n",
    "basinDataSet = pd.read_csv((readSetPath+importName), index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['percentage','DLSOL5R200', 'DLSOL4R150', 'DLSOL5R150']\n",
    "for col in colNames: \n",
    "    basinDataSet[col].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.dropna(subset=['slope'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.drop(['fid'], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basinDataSet.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOrmalize Flow Accumulation\n",
    "basinDataSet['FAcc'] = (basinDataSet['FAcc']- basinDataSet['FAcc'].min())/(basinDataSet['FAcc'].max()-basinDataSet['FAcc'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing QGIS NoData value(-9999) with 0 \n",
    "repalcer  = basinDataSet['FAProx_01'].to_numpy()\n",
    "basinDataSet['FAProx_01'] = [0 if repalcer[j] == -9999 else repalcer[j] for j in range(len(repalcer))]                                                                                                                         \n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0] int32\n"
     ]
    }
   ],
   "source": [
    "## Transform a column datatype\n",
    "repalcer  = basinDataSet['percentage'].to_numpy().astype('int32')\n",
    "print(repalcer[0:10],repalcer.dtype)\n",
    "basinDataSet.loc[:,'percentage'] = repalcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make binary Dataset ###\n",
    "# keep class_0 and replace with 1 all other classes. \n",
    "basinDataSet = ms.makeBinary(basinDataSet,'percentage',0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>FAcc</th>\n",
       "      <th>slope</th>\n",
       "      <th>elevation</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>DLSOL4R150</th>\n",
       "      <th>DLSOL5R150</th>\n",
       "      <th>DLSOL5R200</th>\n",
       "      <th>FAProx_01</th>\n",
       "      <th>FAProx_025</th>\n",
       "      <th>visibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>3.371020e+05</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "      <td>337102.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.015850</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>6.622236</td>\n",
       "      <td>133.232419</td>\n",
       "      <td>359622.664410</td>\n",
       "      <td>5.270253e+06</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>230.534306</td>\n",
       "      <td>60.472845</td>\n",
       "      <td>0.046110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.124894</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>5.237760</td>\n",
       "      <td>32.044910</td>\n",
       "      <td>967.632139</td>\n",
       "      <td>1.218491e+03</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>168.578843</td>\n",
       "      <td>44.464991</td>\n",
       "      <td>0.045132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.047130</td>\n",
       "      <td>358208.000000</td>\n",
       "      <td>5.266260e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775075</td>\n",
       "      <td>119.764798</td>\n",
       "      <td>358938.000000</td>\n",
       "      <td>5.269408e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>93.005380</td>\n",
       "      <td>24.020820</td>\n",
       "      <td>0.017780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.431835</td>\n",
       "      <td>138.491225</td>\n",
       "      <td>359413.000000</td>\n",
       "      <td>5.270243e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>200.249850</td>\n",
       "      <td>51.739730</td>\n",
       "      <td>0.031670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>9.109063</td>\n",
       "      <td>152.126040</td>\n",
       "      <td>360098.000000</td>\n",
       "      <td>5.271088e+06</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>337.342560</td>\n",
       "      <td>88.887570</td>\n",
       "      <td>0.056780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000200</td>\n",
       "      <td>50.883080</td>\n",
       "      <td>227.877210</td>\n",
       "      <td>364270.000000</td>\n",
       "      <td>5.272723e+06</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>268.001860</td>\n",
       "      <td>0.396390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          percentage           FAcc          slope      elevation  \\\n",
       "count  337102.000000  337102.000000  337102.000000  337102.000000   \n",
       "mean        0.015850       0.002284       6.622236     133.232419   \n",
       "std         0.124894       0.036096       5.237760      32.044910   \n",
       "min         0.000000       0.000000       0.001370       0.047130   \n",
       "25%         0.000000       0.000000       2.775075     119.764798   \n",
       "50%         0.000000       0.000010       5.431835     138.491225   \n",
       "75%         0.000000       0.000050       9.109063     152.126040   \n",
       "max         1.000000       1.000200      50.883080     227.877210   \n",
       "\n",
       "             x_coord       y_coord     DLSOL4R150     DLSOL5R150  \\\n",
       "count  337102.000000  3.371020e+05  337102.000000  337102.000000   \n",
       "mean   359622.664410  5.270253e+06       0.001828       0.003774   \n",
       "std       967.632139  1.218491e+03       0.002676       0.003890   \n",
       "min    358208.000000  5.266260e+06       0.000000       0.000000   \n",
       "25%    358938.000000  5.269408e+06       0.000000       0.000000   \n",
       "50%    359413.000000  5.270243e+06       0.000000       0.003040   \n",
       "75%    360098.000000  5.271088e+06       0.003730       0.005930   \n",
       "max    364270.000000  5.272723e+06       0.013170       0.020200   \n",
       "\n",
       "          DLSOL5R200      FAProx_01     FAProx_025     visibility  \n",
       "count  337102.000000  337102.000000  337102.000000  337102.000000  \n",
       "mean        0.003785     230.534306      60.472845       0.046110  \n",
       "std         0.003283     168.578843      44.464991       0.045132  \n",
       "min         0.000000       0.000000       0.000000       0.000020  \n",
       "25%         0.000880      93.005380      24.020820       0.017780  \n",
       "50%         0.003290     200.249850      51.739730       0.031670  \n",
       "75%         0.005910     337.342560      88.887570       0.056780  \n",
       "max         0.016340     900.000000     268.001860       0.396390  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basinDataSet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>FAcc</th>\n",
       "      <th>slope</th>\n",
       "      <th>elevation</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>DLSOL4R150</th>\n",
       "      <th>DLSOL5R150</th>\n",
       "      <th>DLSOL5R200</th>\n",
       "      <th>FAProx_01</th>\n",
       "      <th>FAProx_025</th>\n",
       "      <th>visibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.97503</td>\n",
       "      <td>174.32889</td>\n",
       "      <td>362010</td>\n",
       "      <td>5271820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>725.43091</td>\n",
       "      <td>174.75125</td>\n",
       "      <td>0.02455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7.28373</td>\n",
       "      <td>173.66133</td>\n",
       "      <td>362010</td>\n",
       "      <td>5271800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>709.11920</td>\n",
       "      <td>171.02631</td>\n",
       "      <td>0.02849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>3.28983</td>\n",
       "      <td>173.39836</td>\n",
       "      <td>362030</td>\n",
       "      <td>5271820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>737.05499</td>\n",
       "      <td>176.23280</td>\n",
       "      <td>0.02651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36284</td>\n",
       "      <td>172.95361</td>\n",
       "      <td>362030</td>\n",
       "      <td>5271800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>721.00623</td>\n",
       "      <td>172.53986</td>\n",
       "      <td>0.02117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.19720</td>\n",
       "      <td>172.93945</td>\n",
       "      <td>362050</td>\n",
       "      <td>5271820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>749.03271</td>\n",
       "      <td>177.79201</td>\n",
       "      <td>0.00134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percentage     FAcc    slope  elevation  x_coord  y_coord  DLSOL4R150  \\\n",
       "0           0  0.00000  1.97503  174.32889   362010  5271820         0.0   \n",
       "1           0  0.00000  7.28373  173.66133   362010  5271800         0.0   \n",
       "2           0  0.00001  3.28983  173.39836   362030  5271820         0.0   \n",
       "3           0  0.00000  0.36284  172.95361   362030  5271800         0.0   \n",
       "4           0  0.00000  0.19720  172.93945   362050  5271820         0.0   \n",
       "\n",
       "   DLSOL5R150  DLSOL5R200  FAProx_01  FAProx_025  visibility  \n",
       "0         0.0         0.0  725.43091   174.75125     0.02455  \n",
       "1         0.0         0.0  709.11920   171.02631     0.02849  \n",
       "2         0.0         0.0  737.05499   176.23280     0.02651  \n",
       "3         0.0         0.0  721.00623   172.53986     0.02117  \n",
       "4         0.0         0.0  749.03271   177.79201     0.00134  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basinDataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportName = 'MLPBinary_basin2_CleanDataSet_VsAll_Test.csv'\n",
    "basinDataSet.to_csv((saveDatasetPath+exportName), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS.head(5)\n",
    "s = {}\n",
    "s['Datas'] = ds\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportional Spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X,Y = ms.importDataSet('datasets/basin1_FirstFeatureSet_Clean.csv', 'percentage')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=50)\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    print(\"TRAIN:\", train_index.size, \"TEST:\", test_index.size)\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = Y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = Y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describing training set\n",
    "print(len(X_train['elevation']), len(y_train) )\n",
    "trainCount = Counter(y_train)\n",
    "print(trainCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####    Creating training set     #####\n",
    "X_train.loc[:,'percentage'] = y_train\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing coordinates from training set\n",
    "X_train.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('datasets/basin1_FirstFeatureSet_Clean_Training.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####. Creating Test set\n",
    "print(X_test.head())\n",
    "X_test.loc[:,'percentage'] = y_test\n",
    "print(X_test.head())\n",
    "print(X_test.info())\n",
    "testCount = Counter(X_test['percentage'])\n",
    "print(f\"testCount:  {testCount}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('datasets/basin1_FirstFeatureSet_Clean_Test.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This proportions are the reason why a sample_weight of 0.01 for the majority class give best results for regression\n",
    "totalTrain = sum([trainCount[0], trainCount[1], trainCount[5]]) \n",
    "totalValidation = sum([testCount[0], testCount[1], testCount[5]])\n",
    "print(f\"total Train samples: {totalTrain},  total Validation samples: {totalValidation}\")\n",
    "print(\"Summary of traning and test dataset class balance\")\n",
    "print(f\"Training Set:\", '\\n', \"Class 0: %.3f\" %(trainCount[0]/totalTrain), \" Class 1: %.4f\" %(trainCount[1]/totalTrain), \"Class 5: %.4f\"%(trainCount[5]/totalTrain))\n",
    "print(\"Testing Set:\", '\\n', \"Class 0: %.3f\" %(testCount[0]/totalValidation),\" Class 1: %.4f\" %(testCount[1]/totalValidation),  \"Class 5: %.4f\"%(testCount[5]/totalValidation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms.loadModel('./outputs/2022-08-05/00-35-58/2208050035.pkl')\n",
    "dataSetToSave = ms.makePredictionToImportAsSHP(csvName, model, X, Y, 'percentage')\n",
    "print(dataSetToSave.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining dataSets to build AllVsOne_training and OneVsAll_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat datasets\n",
    "sourceFileForDatasets = 'datasets/'\n",
    "allDataSetsNames = ['basin1Light_Clean.csv', 'basin2_CleanDataSet.csv', 'basin3_CleanDataSet_copy.csv','basin4_CleanDataSet.csv','basin5_CleanDataSet.csv']\n",
    "# OneVsAllDataSetName = 'basin1Light_Clean.csv'\n",
    "\n",
    "for datasetForTest in allDataSetsNames:\n",
    "    filename, file_extension = os.path.splitext(datasetForTest)\n",
    "    newListOfNames = [s for s in allDataSetsNames if s != datasetForTest]\n",
    "    allDataSetsFileName = 'allVs_'+ filename +'_Training'\n",
    "    DFToConcatAll = pd.DataFrame()\n",
    "#     DFToConcatAll = pd.read_csv((sourceFileForDatasets+datasetForTest), index_col = None)\n",
    "#     print(DFToConcatAll.head())\n",
    "    for datasets in newListOfNames:\n",
    "        DFToConcatAll = pd.concat([DFToConcatAll, pd.read_csv((sourceFileForDatasets+datasets), index_col = None)])\n",
    "    nameToSafe = sourceFileForDatasets+allDataSetsFileName+file_extension\n",
    "    DFToConcatAll.drop(['x_coord','y_coord'], axis =1, inplace=True)\n",
    "    DFToConcatAll.to_csv(nameToSafe, index=None)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destiationPath = 'datasets/RFdatasets'\n",
    "listFile = os.listdir(destiationPath)\n",
    "print(listFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testList = ['basin1Light_Clean_Test.csv','basin2_Test.csv','basin3_Test.csv','basin4_Test.csv','basin5_Test.csv']\n",
    "traininList = ['basin1Light_Clean_Training.csv','basin2_Training.csv','basin3_Training.csv','basin4_Training.csv','basin5_Training.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning datasets: Removing not usefull variables from All_VS_ONE \n",
    "readPath = 'datasets/RFdatasets/'\n",
    "destiationPath = 'datasets/dataset4MLP/'\n",
    "datasetNamelist = ['basin1Light_Clean_Test.csv','basin2_Test.csv','basin3_Test.csv','basin4_Test.csv','basin5_Test.csv',\n",
    "                  'basin1Light_Clean_Training.csv','basin2_Training.csv','basin3_Training.csv','basin4_Training.csv',\n",
    "                   'basin5_Training.csv']\n",
    "featuresToDelete = ['TPI','TWI']\n",
    "for i in datasetNamelist:\n",
    "    path = readPath + i \n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    basinDataSet.drop(featuresToDelete, axis=1, inplace=True)\n",
    "    savePath = destiationPath + 'MLP_'+ i\n",
    "    basinDataSet.to_csv(savePath, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###. Build dataset subset for MLP test (Only first 150K samples)\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "datasetNamelist = ['MLP_allVs_basin1Light_Clean_Training.csv','MLP_basin1Light_Clean_VsAll_Test.csv']\n",
    "for i in datasetNamelist:\n",
    "    path = readPath + i \n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    Y = np.array(basinDataSet['percentage'])\n",
    "    count,_ = md.listClassCountPercent(Y)\n",
    "    basinDataSet.drop(basinDataSet.loc[150000:count].index,axis=0,inplace=True)\n",
    "    savePath = readPath + 'reduced_'+i\n",
    "    basinDataSet.to_csv(savePath, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring datasets\n",
    "dataset = ['reduced_MLP_allVs_basin1Light_Clean_Training.csv','reduced_MLP_basin1Light_Clean_VsAll_Test.csv']\n",
    "for i in dataset:\n",
    "    path = readPath + i \n",
    "    print(path)\n",
    "    basinDataSet = pd.read_csv(path, index_col = None)\n",
    "    print(basinDataSet.head())\n",
    "    Y = np.array(basinDataSet['percentage'])\n",
    "    print(md.listClassCountPercent(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.  Training TEST\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "trainingPath = readPath + 'MLP_allVs_basin1Light_Clean_Training.csv'\n",
    "params = {'random_state':50, 'hidden_layer_sizes': 2,\n",
    "                'early_stopping':True,'max_iter':200,'verbose':False,\n",
    "                'tol':0.00010,'validation_fraction':0.1,'warm_start':False}\n",
    "mlpc = md.implementingMLPCalssifier(trainingPath,'percentage',params)\n",
    "mlpc.fitMLPClassifier()\n",
    "mlpc.plotLossBehaviour()\n",
    "\n",
    "mlpClassifier = mlpc.getMLPClassifier()\n",
    "\n",
    "# #Validating un unseen datase\n",
    "# validation = readPath + 'MLP_basin1Light_Clean_VsAll_Test.csv'\n",
    "# x_val,y_val = ms.importDataSet(validation, 'percentage')\n",
    "# prediction = ms.makePredictionToImportAsSHP(mlpClassifier, x_val, y_val, 'percentage')\n",
    "\n",
    "# ## Compute metrics\n",
    "# X = x_val.copy()\n",
    "# X.drop(['x_coord','y_coord'], axis=1, inplace=True)\n",
    "# metrics = md.computeClassificationMetrics(mlpClassifier,X,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlpc.get_logsDic())\n",
    "mlpc.logMLPClassifier({'test':34})\n",
    "print(mlpc.get_logsDic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(('outputs/'+ 'MLP_basin1Light_firstResult_HL280.csv'),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement bets hiddenLayerSize exploration\n",
    "readPath = 'datasets/dataset4MLP/'\n",
    "trainingPath = readPath + 'MLP_allVs_basin1Light_Clean_Training.csv'\n",
    "validation = readPath + 'MLP_basin1Light_Clean_VsAll_Test.csv'\n",
    "# dataset = pd.read_csv(trainingPath, index_col = None)\n",
    "params = {'random_state':50, 'hidden_layer_sizes': 2,\n",
    "                'early_stopping':False,'max_iter':2,'verbose':True,\n",
    "                'tol':0.00010,'validation_fraction':0.1,'warm_start':False}\n",
    "mlpc = md.implementingMLPCalssifier(trainingPath,'percentage',params)\n",
    "\n",
    "x_val,Y_val = ms.importDataSet(validation, 'percentage')\n",
    "X = x_val.copy()\n",
    "X.drop(['x_coord','y_coord'], axis=1, inplace=True)\n",
    "firstInterval = np.arange(100,1009,100)\n",
    "mlpc.explore4BestHLSize(X,Y_val,firstInterval,'5',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controled sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = pd.read_csv('datasets/basin2 _Training.csv', index_col = None)\n",
    "print(DS.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(DS['FAProx_01']) # , , DS['elevation'], DS['disToRiv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resampling appliying class selection by rule:\n",
    "\n",
    "# RULE1: Select point at a distance to river less than 300m. \n",
    "\n",
    "# # newDS = pseudoClassCreation(DS, \"distanceToRiver\", 300, 2)\n",
    "def pseudoClassCreation(dataset, conditionVariable, threshold, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Replace <targetClass> by  <pseudoClass> where <conditionVariable >= threshold>. \n",
    "    Return:\n",
    "      dataset with new classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    conditionVar = (np.array(dataset[conditionVariable])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ pseudoClass if conditionVar[j] >= threshold \n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "def revertPseudoClassCreation(dataset, originalClass, pseudoClass, targetClassName):\n",
    "    '''\n",
    "    Restablich  <targetClass> with <originalClass> where <targetClassName == pseudoClass>. \n",
    "    Return:\n",
    "      dataset with original classes group. \n",
    "    '''\n",
    "    datsetReclassified = dataset.copy()\n",
    "    actualTarget = (np.array(dataset[targetClassName])).ravel()\n",
    "    datsetReclassified[targetClassName] = [ originalClass if actualTarget[j] == pseudoClass\n",
    "                                           else actualTarget[j]\n",
    "                                           for j in range(len(actualTarget))]\n",
    "    print(Counter(datsetReclassified[targetClassName]))\n",
    "    return  datsetReclassified\n",
    "\n",
    "\n",
    "print(Counter(X_train['percentage']))\n",
    "newDS = pseudoClassCreation(X_train, 'disToRiv', 200, 2, 'percentage')\n",
    "y = newDS['percentage']\n",
    "newDS.drop(['percentage'], axis=1, inplace = True)\n",
    "x_res,y_res = ms.randomUndersampling(newDS, y, )\n",
    "x_res['percentage'] = y_res\n",
    "# newDatase = revertPseudoClassCreation(x_res, 0, 2, 'percentage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res.to_csv('basin1ControlClass0Sampling4Class_ToSHP.csv',index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import dataset to describe\n",
    "DS= pd.read_csv('datasets/basin4_Training.csv', index_col=None)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.drop(['x_coord','y_coord'], axis = 1, inplace=True)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAcc vs Labels\n",
    "targets = DS['percentage']\n",
    "FAcc = original['FAcc']\n",
    "FAcc_norm = DS['FAcc_norm']\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13,4), sharey=True)\n",
    "fig.text(-0.02, 0.5, 'labels', va='center', rotation='vertical')\n",
    "fig.text(0.5, 1, 'Flow accumulation vs labels distribution', ha ='center')\n",
    "axs[0].scatter(FAcc,targets)\n",
    "# axs[0].set_title(\"Facc\")\n",
    "axs[0].set(xlabel='a) Flow Accumulation')\n",
    "axs[1].scatter(FAcc_norm,targets)\n",
    "# axs[1].set_title(\"FAcc_norm\")\n",
    "axs[1].set(xlabel='b) Flow Accumulation estandardized')\n",
    "plt.rcParams['font.size'] = '20'\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Plot all features vs labels\n",
    "# 'disToRiv', 'TWI', 'TPI', 'slope', 'elevation',\n",
    "\n",
    "targets = DS['percentage']\n",
    "# targets = np.where(targets == 5,2,targets)\n",
    "\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['LDSOL4R150']\n",
    "DLSOL5R150 = DS['LDSOL5R150']\n",
    "DLSOL5R200 = DS['LDSOL5R200']\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "\n",
    "fig, axs = plt.subplots(4,3, figsize=(13, 8), sharey=True)\n",
    "fig.supylabel('Labels')\n",
    "plt.rcParams['font.size'] = '15'\n",
    "plt.yticks([0,1,5])\n",
    "\n",
    "'''\n",
    "E = DS['elevation'] \n",
    "slope = DS['slope']\n",
    "FAcc = DS['FAcc']\n",
    "TWI = DS['TWI']\n",
    "'''\n",
    "axs[0, 0].scatter(E,targets)\n",
    "axs[0, 0].set_title(\"Elevation\")\n",
    "axs[1, 0].scatter(slope,targets)\n",
    "axs[1, 0].set_title(\"Slope\")\n",
    "axs[2, 0].scatter(FAcc,targets)\n",
    "axs[2, 0].set_title(\"Flow accumulation\")\n",
    "axs[3, 0].scatter(TWI,targets)\n",
    "axs[3, 0].set_title(\"TWI\")\n",
    "\n",
    "'''\n",
    "TPI = DS['TPI']\n",
    "DLSOL4R150 = DS['DLSOL4R150']\n",
    "DLSOL5R150 = DS['DLSOL5R150']\n",
    "DLSOL5R200 = DS['DLSOL5R200']\n",
    "'''\n",
    "axs[0, 1].scatter(TPI,targets)\n",
    "axs[0, 1].set_title('TPI')\n",
    "axs[1, 1].scatter(DLSOL4R150,targets)\n",
    "axs[1, 1].set_title(\"DLSOL4R150\")\n",
    "axs[2, 1].scatter(DLSOL5R150,targets)\n",
    "axs[2, 1].set_title(\"DLSOL5R150\")\n",
    "axs[3, 1].scatter(DLSOL5R200,targets)\n",
    "axs[3, 1].set_title(\"DLSOL5R200\")\n",
    "\n",
    "'''\n",
    "FAProx_01 = DS['FAProx_01']\n",
    "FAProx_025 = DS['FAProx_025']\n",
    "visibility = DS['visibility']\n",
    "'''\n",
    "axs[0, 2].scatter(FAProx_01,targets)\n",
    "axs[0, 2].set_title('FAProx_01')\n",
    "axs[1, 2].scatter(FAProx_025,targets)\n",
    "axs[1, 2].set_title(\"FAProx_025\")\n",
    "axs[2, 2].scatter(visibility,targets)\n",
    "axs[2, 2].set_title(\"Visibility\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS.head())\n",
    "#  Return a dataset with the rows corresponding to the index where condition in DS.columName is valid. \n",
    "dsArray = DS[DS.percentage != 0] \n",
    "print(dsArray.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.pairplot(DS, hue = 'percentage', diag_kind = 'kde', \n",
    "             plot_kws = {'alpha': 0.8, 's': 100},\n",
    "             height = 4, corner=True, palette = \"Set2\")# vars = ['life_exp', 'log_pop', 'log_gdp_per_cap'],\n",
    "\n",
    "# sns.pairplot(DS, hue=\"percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####. Covariance Matrix\n",
    "sns.set(font_scale=0.7)\n",
    "matrix = DS.corr().round(2)\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.set_figsize=(25,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(criterion='entropy', random_state = 50)\n",
    "x_train,y_train = ms.importDataSet('basin1Train.csv', 'percentage')\n",
    "classifier = OneVsRestClassifier(estimator).fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ms.loadModel('./outputs/2022-08-05/11-01-57/2208051101.pkl')\n",
    "x_test,y_test = ms.importDataSet('basin1Test.csv', 'percentage')\n",
    "\n",
    "x_test = ms.removeCoordinatesFromDataSet(x_test)\n",
    "\n",
    "# y_prob = classifier.predict_proba(x_test)\n",
    "#print(np.unique(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.plot_ROC_AUC_OneVsRest(classifier, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,y_test = ms.importDataSet('./bestModels/Classifier/10-18-08/2208051018prediction_basin1Test.csv', 'prediction')\n",
    "unique, count = np.unique(y_test, return_counts=True)\n",
    "total = count.sum()\n",
    "print(total)\n",
    "percent = np.round(np.zeros_like(unique).astype('float16'),3)\n",
    "print('values, counts , percent')\n",
    "for i in range(len(unique)):    \n",
    "   percent[i] = (count[i]/total)*100\n",
    "   print(unique[i],\"\\t\", count[i], percent[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhiteBoxTool applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whiteboxApplications as wbtapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = '/Users/abdielfer/DESS/Internship2022/RNCanWork/FloodMaps/WBTWorkDir'\n",
    "rtools = wbtapp.rasterTools(wdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    rtools.gaussianFilter('basin1_FAccDInfLogGauss2.tif', 'basin1_FAccDInfLogGauss2.tif', sigma = 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c444c068df4da933f83035bc75b2ed5aea8e7fb8bc113357e2f14ee7cca6a8fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
